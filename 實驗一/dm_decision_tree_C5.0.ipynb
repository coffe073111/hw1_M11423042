{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b22291f-2828-43ad-b4a3-e0985244131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的目標欄位: class\n",
      "train 最後5個欄位: ['capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "test  最後5個欄位: ['capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "[C5.0][step 1] Train=0.8520 Test=0.8533 Δ=-0.0013 params={'max_depth': 10, 'max_leaf_nodes': 64, 'min_samples_split': 20, 'min_samples_leaf': 10, 'min_gain_ratio': 0.0001, 'cf': 0.25, 'subtree_raising': True, 'random_state': 42}\n",
      "[C5.0] 調參歷程已輸出：c50_tune_log.csv\n",
      "\n",
      "=== C5.0 最終模型（inline tuned）===\n",
      "Train_Acc= 0.8520453637397425\n",
      "Test_Acc = 0.853342344556402\n",
      "Split @[capital-gain <= 7073.5000]  GR=0.3400  n=32537\n",
      " ├─ True:\n",
      "  Split @[capital-gain <= 5119.0000]  GR=0.0852  n=31138\n",
      "   ├─ True:\n",
      "    Split @[age <= 24.5000]  GR=0.0733  n=29378\n",
      "     ├─ True:\n",
      "      Leaf[n=5511] -> <=50K  counts={'<=50K': np.float64(5473.0), '>50K': np.float64(38.0)}\n",
      "     └─ False:\n",
      "      Leaf[n=23867] -> <=50K  counts={'<=50K': np.float64(18648.0), '>50K': np.float64(5219.0)}\n",
      "   └─ False:\n",
      "    Split @[capital-gain <= 5316.5000]  GR=0.5757  n=174\n",
      "     ├─ True:\n",
      "      Leaf[n=93] -> >50K  counts={'>50K': np.float64(93.0)}\n",
      "     └─ False:\n",
      "      Leaf[n=81] -> <=50K  counts={'<=50K': np.float64(62.0), '>50K': np.float64(19.0)}\n",
      " └─ False:\n",
      "  Leaf[n=1399] -> >50K  counts={'>50K': np.float64(1379.0), '<=50K': np.float64(20.0)}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# C5.0 (custom): gain-ratio + pessimistic pruning + raising\n",
    "# ==========================================================\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import copy\n",
    "\n",
    "# ---------- helpers\n",
    "\n",
    "def entropy_from_counts(counts: Dict[Any, float]) -> float:\n",
    "    n = float(sum(counts.values()))\n",
    "    if n <= 0: return 0.0\n",
    "    ent = 0.0\n",
    "    for c in counts.values():\n",
    "        if c > 0:\n",
    "            p = c / n\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def majority_label(counts: Dict[Any, float]) -> Any:\n",
    "    return max(counts.items(), key=lambda kv: kv[1])[0] if counts else None\n",
    "\n",
    "def _norm_ppf(p: float) -> float:\n",
    "    # Acklam approximation of inverse normal CDF\n",
    "    a = [-3.969683028665376e+01,  2.209460984245205e+02, -2.759285104469687e+02,\n",
    "          1.383577518672690e+02, -3.066479806614716e+01,  2.506628277459239e+00]\n",
    "    b = [-5.447609879822406e+01,  1.615858368580409e+02, -1.556989798598866e+02,\n",
    "          6.680131188771972e+01, -1.328068155288572e+01]\n",
    "    c = [-7.784894002430293e-03, -3.223964580411365e-01, -2.400758277161838e+00,\n",
    "         -2.549732539343734e+00,  4.374664141464968e+00,  2.938163982698783e+00]\n",
    "    d = [ 7.784695709041462e-03,  3.224671290700398e-01,  2.445134137142996e+00,\n",
    "          3.754408661907416e+00]\n",
    "    plow, phigh = 0.02425, 1 - 0.02425\n",
    "\n",
    "    if p < plow:\n",
    "        q = math.sqrt(-2 * math.log(p))\n",
    "        num = (((((c[0]*q + c[1])*q + c[2])*q + c[3])*q + c[4])*q + c[5])\n",
    "        den = ((((d[0]*q + d[1])*q + d[2])*q + d[3])*q + 1.0)\n",
    "        return num / den\n",
    "\n",
    "    if p > phigh:\n",
    "        q = math.sqrt(-2 * math.log(1 - p))\n",
    "        num = (((((c[0]*q + c[1])*q + c[2])*q + c[3])*q + c[4])*q + c[5])\n",
    "        den = ((((d[0]*q + d[1])*q + d[2])*q + d[3])*q + 1.0)\n",
    "        return - (num / den)\n",
    "\n",
    "    q = p - 0.5\n",
    "    r = q * q\n",
    "    num = (((((a[0]*r + a[1])*r + a[2])*r + a[3])*r + a[4])*r + a[5]) * q\n",
    "    den = (((((b[0]*r + b[1])*r + b[2])*r + b[3])*r + b[4]) * r + 1.0)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def pessimistic_error_upper(e: float, n: float, cf: float) -> float:\n",
    "    if n <= 0: return 0.0\n",
    "    f = e / n\n",
    "    z = _norm_ppf(1 - cf)   # cf=0.25 -> z≈0.674\n",
    "    denom = 1 + (z*z)/n\n",
    "    centre = f + (z*z)/(2*n)\n",
    "    adj = z * math.sqrt((f*(1-f) + (z*z)/(4*n))/n)\n",
    "    return (centre + adj)/denom\n",
    "\n",
    "# ---------- tree node\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    is_leaf: bool\n",
    "    prediction: Any\n",
    "    depth: int\n",
    "    n_samples: int\n",
    "    class_counts: Dict[Any, float]\n",
    "    # split info\n",
    "    feature: Optional[str] = None\n",
    "    threshold: Optional[float] = None          # numeric\n",
    "    branches: Optional[Dict[Any, \"Node\"]] = None  # categorical\n",
    "    left: Optional[\"Node\"] = None              # numeric\n",
    "    right: Optional[\"Node\"] = None             # numeric\n",
    "    default_child: Optional[Any] = None        # categorical unseen routing\n",
    "    gain_ratio: float = 0.0\n",
    "    # bookkeeping for raising\n",
    "    idx: Optional[np.ndarray] = None           # training indices that reach this node\n",
    "\n",
    "# ---------- main C5.0\n",
    "\n",
    "class C50DecisionTree:\n",
    "    def __init__(self,\n",
    "                 max_depth: Optional[int]=None,\n",
    "                 max_leaf_nodes: Optional[int]=None,\n",
    "                 min_samples_split: int=20,\n",
    "                 min_samples_leaf: int=10,\n",
    "                 min_gain_ratio: float=0.0,\n",
    "                 cf: float=0.25,\n",
    "                 subtree_raising: bool=True,\n",
    "                 class_weight: Optional[Dict[Any, float]]=None,\n",
    "                 random_state: int=42,\n",
    "                 tie_eps: float=1e-12,\n",
    "                 viz_max_depth: Optional[int]=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_gain_ratio = min_gain_ratio\n",
    "        self.cf = cf\n",
    "        self.subtree_raising = subtree_raising\n",
    "        self.class_weight = class_weight or {}\n",
    "        self.random_state = random_state\n",
    "        self.tie_eps = tie_eps\n",
    "        self.viz_max_depth = viz_max_depth\n",
    "\n",
    "        self._tree_: Optional[Node] = None\n",
    "        self._leaf_count = 0\n",
    "        self._feature_types: Dict[str, str] = {}\n",
    "        self._num_median: Dict[str, float] = {}\n",
    "        self._cat_unknown_token = \"Unknown\"\n",
    "\n",
    "        # keep training data for raising evaluation\n",
    "        self._X: Optional[pd.DataFrame] = None\n",
    "        self._y: Optional[np.ndarray] = None\n",
    "        self._w: Optional[np.ndarray] = None\n",
    "\n",
    "    # ---- preprocessing\n",
    "\n",
    "    def _infer_types(self, df: pd.DataFrame):\n",
    "        for c in df.columns:\n",
    "            self._feature_types[c] = 'numeric' if pd.api.types.is_numeric_dtype(df[c]) else 'categorical'\n",
    "\n",
    "    def _fit_imputers(self, X: pd.DataFrame):\n",
    "        for c, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                self._num_median[c] = float(pd.to_numeric(X[c], errors='coerce').median())\n",
    "\n",
    "    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X2 = X.copy()\n",
    "        for c, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                X2[c] = pd.to_numeric(X2[c], errors='coerce').fillna(self._num_median[c])\n",
    "            else:\n",
    "                X2[c] = X2[c].astype('object').where(X2[c].notna(), self._cat_unknown_token)\n",
    "        return X2\n",
    "\n",
    "    # ---- gain-ratio split search\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: np.ndarray, w: np.ndarray) -> Tuple[float, dict]:\n",
    "        n = len(y)\n",
    "        # weighted parent counts\n",
    "        parent_counts = Counter()\n",
    "        for yi, wi in zip(y, w): parent_counts[yi] += wi\n",
    "        parent_entropy = entropy_from_counts(parent_counts)\n",
    "\n",
    "        best_gr, best_spec = -1.0, None\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        for col, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                xs = X[col].values\n",
    "                order = np.argsort(xs)\n",
    "                xs_sorted, y_sorted, w_sorted = xs[order], y[order], w[order]\n",
    "                uniq = np.unique(xs_sorted)\n",
    "                if len(uniq) <= 1: continue\n",
    "                thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "                left_counts = Counter(); left_w = 0.0\n",
    "                total_counts = Counter()\n",
    "                total_w = float(w_sorted.sum())\n",
    "                for yi, wi in zip(y_sorted, w_sorted): total_counts[yi] += wi\n",
    "\n",
    "                ptr = 0\n",
    "                for thr in thresholds:\n",
    "                    while ptr < n and xs_sorted[ptr] <= thr:\n",
    "                        left_counts[y_sorted[ptr]] += w_sorted[ptr]\n",
    "                        left_w += w_sorted[ptr]\n",
    "                        ptr += 1\n",
    "                    nL, nR = left_w, total_w - left_w\n",
    "                    if nL < self.min_samples_leaf or nR < self.min_samples_leaf:\n",
    "                        continue\n",
    "                    right_counts = {k: total_counts[k] - left_counts[k] for k in total_counts}\n",
    "                    gain = parent_entropy \\\n",
    "                        - (nL/total_w)*entropy_from_counts(left_counts) \\\n",
    "                        - (nR/total_w)*entropy_from_counts(right_counts)\n",
    "                    split_info = 0.0\n",
    "                    for m in (nL, nR):\n",
    "                        p = m/total_w\n",
    "                        if p > 0: split_info -= p*math.log2(p)\n",
    "                    if split_info <= 1e-12: continue\n",
    "                    gr = gain / split_info\n",
    "                    # tie-break slightly random within eps\n",
    "                    if gr > best_gr + self.tie_eps or (abs(gr - best_gr) <= self.tie_eps and rng.rand() < 0.5):\n",
    "                        best_gr, best_spec = gr, dict(kind='numeric', feature=col, threshold=float(thr))\n",
    "            else:\n",
    "                groups = defaultdict(list)\n",
    "                for i, v in enumerate(X[col].values): groups[v].append(i)\n",
    "                if len(groups) <= 1: continue\n",
    "\n",
    "                valid = True\n",
    "                child_entropy = 0.0\n",
    "                total_w = float(w.sum())\n",
    "                for idxs in groups.values():\n",
    "                    wk = float(w[idxs].sum())\n",
    "                    if wk < self.min_samples_leaf: valid = False; break\n",
    "                    ck = Counter()\n",
    "                    for ii in idxs: ck[y[ii]] += w[ii]\n",
    "                    child_entropy += (wk/total_w)*entropy_from_counts(ck)\n",
    "                if not valid: continue\n",
    "                gain = parent_entropy - child_entropy\n",
    "                split_info = entropy_from_counts(Counter({k: float(w[idxs].sum()) for k, idxs in groups.items()}))\n",
    "                if split_info <= 1e-12: continue\n",
    "                gr = gain / split_info\n",
    "                if gr > best_gr + self.tie_eps or (abs(gr - best_gr) <= self.tie_eps and rng.rand() < 0.5):\n",
    "                    best_gr = gr\n",
    "                    best_spec = dict(kind='categorical', feature=col,\n",
    "                                     groups={k: np.asarray(v, dtype=int) for k, v in groups.items()})\n",
    "        return best_gr, best_spec\n",
    "\n",
    "    # ---- build\n",
    "\n",
    "    def _weighted_counts(self, y, w) -> Dict[Any, float]:\n",
    "        c = Counter()\n",
    "        for yi, wi in zip(y, w):\n",
    "            c[yi] += wi * (self.class_weight.get(yi, 1.0))\n",
    "        return c\n",
    "\n",
    "    def _build(self, X: pd.DataFrame, y: np.ndarray, w: np.ndarray, depth: int, idx: np.ndarray) -> Node:\n",
    "        counts = self._weighted_counts(y, w)\n",
    "        pred = majority_label(counts)\n",
    "        total_w = float(w.sum())\n",
    "\n",
    "        # stop\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (self.max_leaf_nodes is not None and self._leaf_count >= self.max_leaf_nodes) or \\\n",
    "           total_w < self.min_samples_split or \\\n",
    "           len(counts) == 1:\n",
    "            self._leaf_count += 1\n",
    "            return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "\n",
    "        best_gr, spec = self._best_split(X, y, w)\n",
    "        if spec is None or best_gr <= self.min_gain_ratio:\n",
    "            self._leaf_count += 1\n",
    "            return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "\n",
    "        # split\n",
    "        if spec['kind'] == 'numeric':\n",
    "            f, thr = spec['feature'], spec['threshold']\n",
    "            mask = (X[f].values <= thr)\n",
    "            XL, yL, wL, idxL = X[mask], y[mask], w[mask], idx[mask]\n",
    "            XR, yR, wR, idxR = X[~mask], y[~mask], w[~mask], idx[~mask]\n",
    "            if float(wL.sum()) < self.min_samples_leaf or float(wR.sum()) < self.min_samples_leaf:\n",
    "                self._leaf_count += 1\n",
    "                return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "            left  = self._build(XL, yL, wL, depth+1, idxL)\n",
    "            right = self._build(XR, yR, wR, depth+1, idxR)\n",
    "            node = Node(False, pred, depth, int(total_w), dict(counts),\n",
    "                        feature=f, threshold=thr, left=left, right=right,\n",
    "                        gain_ratio=best_gr, idx=idx)\n",
    "        else:\n",
    "            f, groups = spec['feature'], spec['groups']\n",
    "            branches = {}\n",
    "            # default branch = largest weight child\n",
    "            largest_k = None; largest_w = -1\n",
    "            for k, id_arr in groups.items():\n",
    "                Xi, yi, wi = X.iloc[id_arr], y[id_arr], w[id_arr]\n",
    "                wsum = float(wi.sum())\n",
    "                if wsum < self.min_samples_leaf: continue\n",
    "                branches[k] = self._build(Xi, yi, wi, depth+1, idx[id_arr])\n",
    "                if wsum > largest_w: largest_w, largest_k = wsum, k\n",
    "            node = Node(False, pred, depth, int(total_w), dict(counts),\n",
    "                        feature=f, branches=branches, default_child=largest_k,\n",
    "                        gain_ratio=best_gr, idx=idx)\n",
    "        return node\n",
    "\n",
    "    # ---- pruning + raising\n",
    "\n",
    "    def _subtree_empirical_error(self, node: Node, idx: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"Return (errors, total_w) evaluated on training subset idx with weights/self.class_weight.\"\"\"\n",
    "        if len(idx) == 0: return 0.0, 0.0\n",
    "        Xs = self._X.iloc[idx]\n",
    "        ys = self._y[idx]\n",
    "        ws = self._w[idx]\n",
    "        # predictions\n",
    "        preds = self._predict_batch_rows(Xs, node)\n",
    "        err = 0.0; tot = 0.0\n",
    "        for y_true, y_pred, wi in zip(ys, preds, ws):\n",
    "            w_eff = wi * self.class_weight.get(y_true, 1.0)\n",
    "            tot += w_eff\n",
    "            if y_true != y_pred: err += w_eff\n",
    "        return err, tot\n",
    "\n",
    "    def _prune(self, node: Node) -> Tuple[float, float]:\n",
    "        if node.is_leaf:\n",
    "            e = node.n_samples - node.class_counts.get(node.prediction, 0.0)\n",
    "            return e, float(node.n_samples)\n",
    "\n",
    "        # post-order\n",
    "        if node.branches is not None:\n",
    "            child_err = 0.0; child_w = 0.0\n",
    "            for ch in node.branches.values():\n",
    "                e, w = self._prune(ch)\n",
    "                child_err += e; child_w += w\n",
    "        else:\n",
    "            eL, wL = self._prune(node.left)\n",
    "            eR, wR = self._prune(node.right)\n",
    "            child_err, child_w = eL+eR, wL+wR\n",
    "\n",
    "        # leaf error if collapsed\n",
    "        leaf_err = node.n_samples - node.class_counts.get(node.prediction, 0.0)\n",
    "\n",
    "        child_rate = pessimistic_error_upper(child_err, child_w, self.cf)\n",
    "        leaf_rate  = pessimistic_error_upper(leaf_err,  float(node.n_samples), self.cf)\n",
    "\n",
    "        if leaf_rate <= child_rate + 1e-12:\n",
    "            node.is_leaf = True\n",
    "            node.feature = node.threshold = None\n",
    "            node.branches = None; node.left = node.right = None\n",
    "            return leaf_err, float(node.n_samples)\n",
    "        return child_err, child_w\n",
    "\n",
    "    def _assign_indices(self, node: Node, idx: np.ndarray):\n",
    "        \"\"\"Push parent idx down the subtree to refresh children's idx after raising.\"\"\"\n",
    "        node.idx = idx\n",
    "        if node.is_leaf: return\n",
    "        Xs = self._X.iloc[idx]\n",
    "        if node.branches is not None:\n",
    "            # categorical\n",
    "            buckets = defaultdict(list)\n",
    "            col = node.feature\n",
    "            for i, v in zip(idx, Xs[col].values):\n",
    "                buckets[v].append(i)\n",
    "            for k, ch in node.branches.items():\n",
    "                self._assign_indices(ch, np.array(buckets.get(k, []), dtype=int))\n",
    "        else:\n",
    "            # numeric\n",
    "            col, thr = node.feature, node.threshold\n",
    "            mask = (Xs[col].values <= thr)\n",
    "            self._assign_indices(node.left,  idx[mask])\n",
    "            self._assign_indices(node.right, idx[~mask])\n",
    "\n",
    "    def _try_raising_here(self, node: Node):\n",
    "        \"\"\"Try subtree raising: replace 'node' with one of its children if pessimistic error doesn't get worse.\"\"\"\n",
    "        if node.is_leaf: return\n",
    "        # evaluate current subtree on node.idx\n",
    "        cur_err, cur_w = self._subtree_empirical_error(node, node.idx)\n",
    "        cur_rate = pessimistic_error_upper(cur_err, cur_w, self.cf)\n",
    "\n",
    "        candidates = []\n",
    "        if node.branches is not None:\n",
    "            candidates = list(node.branches.values())\n",
    "        else:\n",
    "            candidates = [node.left, node.right]\n",
    "\n",
    "        for child in candidates:\n",
    "            ch_err, ch_w = self._subtree_empirical_error(child, node.idx)\n",
    "            ch_rate = pessimistic_error_upper(ch_err, ch_w, self.cf)\n",
    "            if ch_rate <= cur_rate - 1e-12:   # strictly better\n",
    "                # raise: copy child's structure into node\n",
    "                # (deep copy to avoid aliasing other references)\n",
    "                clone = copy.deepcopy(child)\n",
    "                node.is_leaf = clone.is_leaf\n",
    "                node.prediction = clone.prediction\n",
    "                node.depth = node.depth  # keep same\n",
    "                node.n_samples = node.n_samples\n",
    "                node.class_counts = node.class_counts\n",
    "                node.feature = clone.feature\n",
    "                node.threshold = clone.threshold\n",
    "                node.branches = clone.branches\n",
    "                node.left = clone.left\n",
    "                node.right = clone.right\n",
    "                node.default_child = clone.default_child\n",
    "                node.gain_ratio = clone.gain_ratio\n",
    "                # re-distribute indices to refreshed structure\n",
    "                self._assign_indices(node, node.idx)\n",
    "                # after one successful raising,可以再嘗試連鎖提升\n",
    "                return self._try_raising_here(node)\n",
    "\n",
    "    def _raising(self, node: Node):\n",
    "        if node.is_leaf: return\n",
    "        # post-order\n",
    "        if node.branches is not None:\n",
    "            for ch in node.branches.values():\n",
    "                self._raising(ch)\n",
    "        else:\n",
    "            self._raising(node.left)\n",
    "            self._raising(node.right)\n",
    "        # then try raising at this node\n",
    "        self._try_raising_here(node)\n",
    "\n",
    "    # ---- API\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, sample_weight: Optional[np.ndarray]=None):\n",
    "        self._X = X.copy()\n",
    "        yy = y.values if isinstance(y, pd.Series) else np.asarray(y)\n",
    "        self._y = yy\n",
    "        self._w = sample_weight if sample_weight is not None else np.ones(len(yy), dtype=float)\n",
    "\n",
    "        self._infer_types(self._X)\n",
    "        self._fit_imputers(self._X)\n",
    "        X2 = self._transform(self._X)\n",
    "\n",
    "        self._leaf_count = 0\n",
    "        idx_all = np.arange(len(yy), dtype=int)\n",
    "        self._tree_ = self._build(X2, yy, self._w, depth=0, idx=idx_all)\n",
    "        # pruning\n",
    "        self._prune(self._tree_)\n",
    "        # raising\n",
    "        if self.subtree_raising:\n",
    "            self._assign_indices(self._tree_, idx_all)\n",
    "            self._raising(self._tree_)\n",
    "        return self\n",
    "\n",
    "    def _predict_row(self, row: pd.Series, node: Node):\n",
    "        while not node.is_leaf:\n",
    "            if node.branches is not None:\n",
    "                v = row[node.feature]\n",
    "                child = node.branches.get(v)\n",
    "                if child is None:\n",
    "                    child = node.branches[node.default_child]\n",
    "                node = child\n",
    "            else:\n",
    "                node = node.left if row[node.feature] <= node.threshold else node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _predict_batch_rows(self, X: pd.DataFrame, node: Optional[Node]=None):\n",
    "        if node is None: node = self._tree_\n",
    "        return [self._predict_row(X.iloc[i], node) for i in range(len(X))]\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        X2 = self._transform(X)\n",
    "        return np.array(self._predict_batch_rows(X2))\n",
    "\n",
    "    # ---- pretty print\n",
    "\n",
    "    def print_tree(self, node: Optional[Node]=None, depth: int=0, max_depth: Optional[int]=None):\n",
    "        if node is None: node = self._tree_\n",
    "        if node is None:\n",
    "            print(\"(empty)\"); return\n",
    "        if max_depth is None: max_depth = self.viz_max_depth\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf or depth >= max_depth:\n",
    "            print(f\"{indent}Leaf[n={node.n_samples}] -> {node.prediction}  counts={node.class_counts}\")\n",
    "            return\n",
    "        if node.branches is not None:\n",
    "            print(f\"{indent}Split @[#{node.feature}]  GR={node.gain_ratio:.4f}  n={node.n_samples}\")\n",
    "            for k, ch in node.branches.items():\n",
    "                print(f\"{indent} ├─ {k}:\")\n",
    "                self.print_tree(ch, depth+1, max_depth)\n",
    "        else:\n",
    "            print(f\"{indent}Split @[{node.feature} <= {node.threshold:.4f}]  GR={node.gain_ratio:.4f}  n={node.n_samples}\")\n",
    "            print(f\"{indent} ├─ True:\");  self.print_tree(node.left,  depth+1, max_depth)\n",
    "            print(f\"{indent} └─ False:\"); self.print_tree(node.right, depth+1, max_depth)\n",
    "\n",
    "# =========================\n",
    "# Dataset(標籤欄位：class）\n",
    "# =========================\n",
    "# ======================\n",
    "# 參數\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH = \"adult_data_no_duplicates.csv\"\n",
    "TEST_PATH  = \"adult_test_no_duplicates.csv\"\n",
    "\n",
    "MAX_DEPTH         = 10\n",
    "MAX_LEAF_NODES    = 64\n",
    "MIN_SAMPLES_SPLIT = 20\n",
    "MIN_SAMPLES_LEAF  = 10         \n",
    "MIN_GAIN          = 1e-4        # C4.5/C5.0：視為 min_gain_ratio 門檻\n",
    "CF                = 0.25        # 悲觀誤差修剪的信賴係數 (C4.5/C5.0)\n",
    "USE_RAISING       = True        # C5.0 子樹提升\n",
    "\n",
    "# ======================\n",
    "# 讀資料（標籤欄位=class）\n",
    "# ======================\n",
    "# ====== 對齊標籤欄（放在讀完 CSV 之後） ======\n",
    "\n",
    "def load_adult(train_path=TRAIN_PATH, test_path=TEST_PATH,\n",
    "               target_candidates=(\"class\",\"income\",\"label\",\"target\",\"y\")):\n",
    "    # 1) 讀檔\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    # 2) 欄名正規化\n",
    "    def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df.columns = [str(c).replace(\"\\ufeff\",\"\").strip().lower() for c in df.columns]\n",
    "        return df\n",
    "    train_df = _norm_cols(train_df)\n",
    "    test_df  = _norm_cols(test_df)\n",
    "\n",
    "    # 3) 由訓練集決定目標欄；測試集改名對齊\n",
    "    target_col = next((c for c in train_df.columns if c in target_candidates),\n",
    "                      train_df.columns[-1])\n",
    "    if target_col not in test_df.columns:\n",
    "        test_df = test_df.rename(columns={test_df.columns[-1]: target_col})\n",
    "\n",
    "    # 4) 標籤正規化（移除句點、統一大小寫）\n",
    "    def _norm_labels(s: pd.Series) -> pd.Series:\n",
    "        if s.dtype == object or s.dtype.name == \"category\":\n",
    "            return (s.astype(str).str.strip()\n",
    "                    .str.replace(r\"\\.$\",\"\", regex=True)\n",
    "                    .str.replace(\">50k\", \">50K\", case=False)\n",
    "                    .str.replace(\"<=50k\",\"<=50K\", case=False))\n",
    "        return s\n",
    "\n",
    "    X_train = train_df.drop(columns=[target_col]).replace(\"?\", np.nan)\n",
    "    y_train = _norm_labels(train_df[target_col])\n",
    "    X_test  = test_df.drop(columns=[target_col]).replace(\"?\", np.nan)\n",
    "    y_test  = _norm_labels(test_df[target_col])\n",
    "\n",
    "    print(\"使用的目標欄位:\", target_col)\n",
    "    print(\"train 最後5個欄位:\", train_df.columns[-5:].tolist())\n",
    "    print(\"test  最後5個欄位:\",  test_df.columns[-5:].tolist())\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# 一鍵取得資料\n",
    "X_train, y_train, X_test, y_test = load_adult()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# C5.0\n",
    "# 需先已定義好 C50DecisionTree 類別\n",
    "# ======================\n",
    "c50 = C50DecisionTree(\n",
    "    max_depth=MAX_DEPTH,\n",
    "    max_leaf_nodes=MAX_LEAF_NODES,\n",
    "    min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "    min_samples_leaf=MIN_SAMPLES_LEAF,\n",
    "    min_gain_ratio=MIN_GAIN,    # ← 用MIN_GAIN 當 gain-ratio 門檻\n",
    "    cf=CF,\n",
    "    subtree_raising=USE_RAISING,\n",
    "    viz_max_depth=3\n",
    ").fit(X_train, y_train)\n",
    "# ========= Auto-tune C5.0 by ΔAcc (inline) =========\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "GAP_THRESHOLD = 0.025\n",
    "MAX_TUNE_STEPS = 5\n",
    "\n",
    "def _fit_eval_c50(_params):\n",
    "    _model = C50DecisionTree(\n",
    "        max_depth=_params.get(\"max_depth\"),\n",
    "        max_leaf_nodes=_params.get(\"max_leaf_nodes\"),\n",
    "        min_samples_split=_params.get(\"min_samples_split\", 20),\n",
    "        min_samples_leaf=_params.get(\"min_samples_leaf\", 10),\n",
    "        min_gain_ratio=_params.get(\"min_gain_ratio\", 1e-4),\n",
    "        cf=_params.get(\"cf\", 0.25),\n",
    "        subtree_raising=_params.get(\"subtree_raising\", True),\n",
    "        random_state=_params.get(\"random_state\", 42),\n",
    "        viz_max_depth=3\n",
    "    ).fit(X_train, y_train)\n",
    "    _tr = accuracy_score(y_train, _model.predict(X_train))\n",
    "    _te = accuracy_score(y_test,  _model.predict(X_test))\n",
    "    return _model, _tr, _te, _tr - _te\n",
    "\n",
    "# 初始參數\n",
    "_init = dict(\n",
    "    max_depth=globals().get(\"MAX_DEPTH\", 10),\n",
    "    max_leaf_nodes=globals().get(\"MAX_LEAF_NODES\", 64),\n",
    "    min_samples_split=globals().get(\"MIN_SAMPLES_SPLIT\", 20),\n",
    "    min_samples_leaf=globals().get(\"MIN_SAMPLES_LEAF\", 10),\n",
    "    min_gain_ratio=globals().get(\"MIN_GAIN\", 1e-4),\n",
    "    cf=globals().get(\"CF\", 0.25),\n",
    "    subtree_raising=globals().get(\"USE_RAISING\", True),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "_hist_rows = []\n",
    "_params = _init.copy()\n",
    "c50_final = None\n",
    "for _step in range(1, MAX_TUNE_STEPS + 1):\n",
    "    _m, _tr, _te, _gap = _fit_eval_c50(_params)\n",
    "    _hist_rows.append({\"step\": _step, \"train_acc\": _tr, \"test_acc\": _te, \"gap\": _gap, **_params})\n",
    "    print(f\"[C5.0][step {_step}] Train={_tr:.4f} Test={_te:.4f} Δ={_gap:.4f} params={_params}\")\n",
    "    if _gap <= GAP_THRESHOLD:\n",
    "        c50_final = _m\n",
    "        break\n",
    "    # 收緊\n",
    "    _params[\"min_gain_ratio\"]    = min(_params.get(\"min_gain_ratio\", 1e-4) * 1.7, 1e-1)\n",
    "    _params[\"min_samples_split\"] = min(int(_params.get(\"min_samples_split\", 20) * 1.25), 300)\n",
    "    _params[\"min_samples_leaf\"]  = min(int(_params.get(\"min_samples_leaf\", 10)  * 1.25), 200)\n",
    "    if _params.get(\"max_depth\", None) is not None:\n",
    "        _params[\"max_depth\"] = max(3, int(_params[\"max_depth\"]) - 1)\n",
    "    if _params.get(\"max_leaf_nodes\", None) is not None:\n",
    "        _params[\"max_leaf_nodes\"] = max(16, int(_params[\"max_leaf_nodes\"] * 0.8))\n",
    "    _params[\"cf\"] = min(0.5, float(_params.get(\"cf\", 0.25)) * 1.1)\n",
    "\n",
    "if c50_final is None:\n",
    "    c50_final = _m\n",
    "\n",
    "import pandas as _pd\n",
    "_c50_hist = _pd.DataFrame(_hist_rows).round(6)\n",
    "_c50_hist.to_csv(\"c50_tune_log.csv\", index=False)\n",
    "print(\"[C5.0] 調參歷程已輸出：c50_tune_log.csv\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score as _acc\n",
    "print(\"\\n=== C5.0 最終模型（inline tuned）===\")\n",
    "print(\"Train_Acc=\", _acc(y_train, c50_final.predict(X_train)))\n",
    "print(\"Test_Acc =\", _acc(y_test,  c50_final.predict(X_test)))\n",
    "\n",
    "try:\n",
    "    c50_final.print_tree(max_depth=3)\n",
    "except Exception as e:\n",
    "    print(\"[warn] print_tree 失敗：\", e)\n",
    "# ========= /Auto-tune C5.0 by ΔAcc (inline) =========\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "648b3788-32bb-45ac-9231-a92742204a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] C5.0 樹圖片：Tree_C50.png\n",
      "[OK] 已輸出：C:\\Users\\rui0731\\anaconda_projects\\0e478e30-a257-4faa-aa14-a67f5fcce6cf\\exports\\Test_Predictions.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === 匯出 C5.0 樹圖（前 3 層；與先前風格一致，Graphviz 優先，無則用 Matplotlib 後備） ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from graphviz import Source\n",
    "    _GV_OK = True\n",
    "except Exception:\n",
    "    _GV_OK = False\n",
    "    print(\"[warn] 找不到 graphviz，將改用 Matplotlib 簡易繪圖。\")\n",
    "\n",
    "def _c50_to_dot(node, view_max_depth=3):\n",
    "    lines = ['digraph Tree {',\n",
    "             'node [shape=box, fontname=\"Helvetica\"];',\n",
    "             'edge [fontname=\"Helvetica\"];']\n",
    "    nid = [0]\n",
    "    def walk(n, depth):\n",
    "        my = nid[0]; nid[0]+=1\n",
    "        if n.is_leaf or depth >= view_max_depth:\n",
    "            lbl = f'leaf\\\\nclass={n.prediction}\\\\nsamples={n.n_samples}'\n",
    "            lines.append(f'{my} [label=\"{lbl}\", style=\"rounded,filled\"];')\n",
    "            return my\n",
    "        if n.branches is not None:\n",
    "            lines.append(f'{my} [label=\"{n.feature}\"];')\n",
    "            for k, ch in n.branches.items():\n",
    "                cid = walk(ch, depth+1)\n",
    "                lab = str(k).replace('\"','\\\\\"')\n",
    "                lines.append(f'{my} -> {cid} [label=\"{lab}\"];')\n",
    "        else:\n",
    "            lines.append(f'{my} [label=\"{n.feature} <= {n.threshold:.4f}\"];')\n",
    "            L = walk(n.left,  depth+1)\n",
    "            R = walk(n.right, depth+1)\n",
    "            lines.append(f'{my} -> {L} [label=\"True\"];')\n",
    "            lines.append(f'{my} -> {R} [label=\"False\"];')\n",
    "        return my\n",
    "    walk((c50_final if \"c50_final\" in globals() and c50_final is not None else c50)._tree_, 0)\n",
    "    lines.append('}')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Matplotlib 後備簡易排版\n",
    "def _c50_count_leaves(n):\n",
    "    if n.is_leaf: return 1\n",
    "    if n.branches is not None:\n",
    "        return sum(_c50_count_leaves(ch) for ch in n.branches.values())\n",
    "    return _c50_count_leaves(n.left) + _c50_count_leaves(n.right)\n",
    "\n",
    "def _c50_layout(n, x0, x1, y, step, coords, edges, labels, depth, view_max_depth):\n",
    "    my_id = id(n); coords[my_id] = ((x0+x1)/2, y, n)\n",
    "    if n.is_leaf or depth >= view_max_depth: return\n",
    "    if n.branches is not None:\n",
    "        kids = list(n.branches.items())\n",
    "        sizes = [ _c50_count_leaves(ch) for _, ch in kids ]\n",
    "        total = sum(sizes)\n",
    "        cur = x0\n",
    "        for (lab, ch), sz in zip(kids, sizes):\n",
    "            w = (x1-x0) * sz / total\n",
    "            nx0, nx1 = cur, cur+w; cur += w\n",
    "            _c50_layout(ch, nx0, nx1, y-step, step, coords, edges, labels, depth+1, view_max_depth)\n",
    "            edges.append((my_id, id(ch))); labels[(my_id, id(ch))] = str(lab)\n",
    "    else:\n",
    "        sizes = [_c50_count_leaves(n.left), _c50_count_leaves(n.right)]\n",
    "        total = sum(sizes); wL = (x1-x0) * sizes[0] / total\n",
    "        _c50_layout(n.left,  x0,     x0+wL, y-step, step, coords, edges, labels, depth+1, view_max_depth)\n",
    "        _c50_layout(n.right, x0+wL, x1,     y-step, step, coords, edges, labels, depth+1, view_max_depth)\n",
    "        edges.append((my_id, id(n.left)));  labels[(my_id, id(n.left))]  = \"True\"\n",
    "        edges.append((my_id, id(n.right))); labels[(my_id, id(n.right))] = \"False\"\n",
    "\n",
    "def save_c50_tree_png(model, out_png=\"Tree_C50.png\", view_max_depth=3):\n",
    "    root = model._tree_\n",
    "    if _GV_OK:\n",
    "        dot = _c50_to_dot(root, view_max_depth=view_max_depth)\n",
    "        src = Source(dot); src.format = \"png\"; src.render(out_png.replace(\".png\",\"\"), cleanup=True)\n",
    "        print(f\"[OK] C5.0 樹圖片：{out_png}\")\n",
    "        return\n",
    "    # 後備：Matplotlib\n",
    "    coords, edges, labels = {}, [], {}\n",
    "    _c50_layout(root, 0.0, 1.0, 1.0, 0.18, coords, edges, labels, 0, view_max_depth)\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    for (x, y, n) in [(*coords[k][:2], coords[k][2]) for k in coords]:\n",
    "        if n.is_leaf:\n",
    "            txt = f'leaf\\nclass={n.prediction}\\nN={n.n_samples}'\n",
    "        else:\n",
    "            txt = n.feature if n.branches is not None else f'{n.feature} <= {n.threshold:.3f}'\n",
    "        ax.text(x, y, txt, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"w\", ec=\"k\"))\n",
    "    for (u,v) in edges:\n",
    "        x1,y1,_ = coords[u]; x2,y2,_ = coords[v]\n",
    "        ax.annotate(\"\", xy=(x2,y2+0.01), xytext=(x1,y1-0.01), arrowprops=dict(arrowstyle=\"-\"))\n",
    "        ax.text((x1+x2)/2, (y1+y2)/2, labels[(u,v)], ha=\"center\", va=\"center\")\n",
    "    ax.set_axis_off(); plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "    print(f\"[OK] C5.0 樹圖片(後備)：{out_png}\")\n",
    "\n",
    "# 取最終模型並輸出樹圖（只畫 3 層）\n",
    "_c50_model = c50_final if \"c50_final\" in globals() and c50_final is not None else c50\n",
    "save_c50_tree_png(_c50_model, out_png=\"Tree_C50.png\", view_max_depth=3)\n",
    "\n",
    "# === 匯出測試結果到 Excel：加入 C5.0 ===\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# 1) 整理 y_true（字串與 0/1）\n",
    "y_true_str = pd.Series(y_test).astype(str).str.strip().str.replace('.', '', regex=False)\n",
    "map01 = {'<=50K': 0, '>50K': 1}\n",
    "y_true01 = y_true_str.map(map01).astype(int).to_numpy()\n",
    "\n",
    "# 2) 各模型預測（若未定義就跳過）\n",
    "def as01(x): \n",
    "    return np.asarray(x).astype(int).ravel()\n",
    "\n",
    "cols = {\n",
    "    \"ID3_pred\":  None,\n",
    "    \"C4.5_pred\": None,\n",
    "    \"CART_pred\": None,\n",
    "    \"C5.0_pred\": None,\n",
    "}\n",
    "\n",
    "\n",
    "# 2b) C5.0 預測（用最終模型）\n",
    "c50_pred_str = pd.Series(_c50_model.predict(X_test)).astype(str).str.strip().str.replace('.', '', regex=False)\n",
    "cols[\"C5.0_pred\"] = c50_pred_str.map(map01).astype(int).to_numpy()\n",
    "\n",
    "# 3) 組合總表（含字串真值 + 各模型 0/1 與是否正確）\n",
    "out = pd.DataFrame({\n",
    "    'row': np.arange(len(y_true01)),\n",
    "    'true': y_true_str,\n",
    "    'true01': y_true01,\n",
    "})\n",
    "for name, arr in cols.items():\n",
    "    if arr is not None:\n",
    "        out[name] = arr\n",
    "        out[name + \"_correct\"] = (arr == y_true01)\n",
    "\n",
    "# 4) 輸出 Excel（優先 openpyxl；無則 xlsxwriter；避免檔案被鎖時自動換名）\n",
    "try:\n",
    "    import openpyxl\n",
    "    _eng = \"openpyxl\"\n",
    "except Exception:\n",
    "    _eng = \"xlsxwriter\"\n",
    "\n",
    "export_dir = Path(\"exports\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "base = export_dir / \"Test_Predictions.xlsx\"\n",
    "\n",
    "def _write_xlsx(path: Path):\n",
    "    with pd.ExcelWriter(path, engine=_eng, mode='w') as xw:\n",
    "        out.to_excel(xw, sheet_name='combined', index=False)\n",
    "        # 分表\n",
    "        for name, arr in cols.items():\n",
    "            if arr is None: \n",
    "                continue\n",
    "            pd.DataFrame({'true01': y_true01, name: arr, 'correct': (arr == y_true01)}).to_excel(\n",
    "                xw, sheet_name=name.replace(\"_pred\",\"\"), index=False\n",
    "            )\n",
    "        # 也輸出 C5.0 字串版方便看\n",
    "        pd.DataFrame({'true': y_true_str, 'C5.0_pred_str': c50_pred_str}).to_excel(\n",
    "            xw, sheet_name='C5.0_str', index=False\n",
    "        )\n",
    "\n",
    "try:\n",
    "    _write_xlsx(base)\n",
    "    print(f\"[OK] 已輸出：{base.resolve()}\")\n",
    "except PermissionError:\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    alt = base.with_name(f\"{base.stem}_{ts}{base.suffix}\")\n",
    "    print(\"[warn] 原檔案可能被 Excel 鎖住，改存新檔。\")\n",
    "    _write_xlsx(alt)\n",
    "    print(f\"[OK] 已輸出：{alt.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b13c427-7663-4f75-bc52-d16a64022075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 混淆矩陣：CM_C50.png\n"
     ]
    }
   ],
   "source": [
    "# === 輸出混淆矩陣圖（依據可用的模型預測自動輸出） ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _save_cm(y_true, y_pred, title, filename, class_names=('< =50K','>50K')):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "    ax.set_title(f\"{title}  Acc={accuracy_score(y_true, y_pred):.4f}\")\n",
    "    plt.tight_layout(); plt.savefig(filename, dpi=300); plt.close()\n",
    "    print(f\"[OK] 混淆矩陣：{filename}\")\n",
    "\n",
    "name_map = {\n",
    "    \"ID3_pred\":  (\"ID3\",  \"CM_ID3.png\"),\n",
    "    \"C4.5_pred\": (\"C4.5\", \"CM_C45.png\"),\n",
    "    \"CART_pred\": (\"CART\", \"CM_CART.png\"),\n",
    "    \"C5.0_pred\": (\"C5.0\", \"CM_C50.png\"),  \n",
    "}\n",
    "\n",
    "for key, (title, fname) in name_map.items():\n",
    "    if key in cols and cols[key] is not None:\n",
    "        _save_cm(y_true01, cols[key], title, fname, class_names=('< =50K','>50K'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b55fdb-63f4-466e-a9b1-4539c5f5c1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
