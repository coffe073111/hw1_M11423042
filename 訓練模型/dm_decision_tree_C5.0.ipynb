{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b22291f-2828-43ad-b4a3-e0985244131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰ΩøÁî®ÁöÑÁõÆÊ®ôÊ¨Ñ‰Ωç: class\n",
      "train ÊúÄÂæå5ÂÄãÊ¨Ñ‰Ωç: ['capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "test  ÊúÄÂæå5ÂÄãÊ¨Ñ‰Ωç: ['capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "\n",
      "=== C5.0Ôºàwith raising=TrueÔºâ ===\n",
      "Train Acc: 0.8520453637397425\n",
      "Test  Acc: 0.853342344556402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.95      0.91     12430\n",
      "        >50K       0.77      0.55      0.64      3846\n",
      "\n",
      "    accuracy                           0.85     16276\n",
      "   macro avg       0.82      0.75      0.77     16276\n",
      "weighted avg       0.85      0.85      0.84     16276\n",
      "\n",
      "Split @[capital-gain <= 7073.5000]  GR=0.3400  n=32537\n",
      " ‚îú‚îÄ True:\n",
      "  Split @[capital-gain <= 5119.0000]  GR=0.0852  n=31138\n",
      "   ‚îú‚îÄ True:\n",
      "    Split @[age <= 24.5000]  GR=0.0733  n=29378\n",
      "     ‚îú‚îÄ True:\n",
      "      Leaf[n=5511] -> <=50K  counts={'<=50K': 5473.0, '>50K': 38.0}\n",
      "     ‚îî‚îÄ False:\n",
      "      Leaf[n=23867] -> <=50K  counts={'<=50K': 18648.0, '>50K': 5219.0}\n",
      "   ‚îî‚îÄ False:\n",
      "    Split @[capital-gain <= 5316.5000]  GR=0.5757  n=174\n",
      "     ‚îú‚îÄ True:\n",
      "      Leaf[n=93] -> >50K  counts={'>50K': 93.0}\n",
      "     ‚îî‚îÄ False:\n",
      "      Leaf[n=81] -> <=50K  counts={'<=50K': 62.0, '>50K': 19.0}\n",
      " ‚îî‚îÄ False:\n",
      "  Leaf[n=1399] -> >50K  counts={'>50K': 1379.0, '<=50K': 20.0}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# C5.0 (custom): gain-ratio + pessimistic pruning + raising\n",
    "# ==========================================================\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import copy\n",
    "\n",
    "# ---------- helpers\n",
    "\n",
    "def entropy_from_counts(counts: Dict[Any, float]) -> float:\n",
    "    n = float(sum(counts.values()))\n",
    "    if n <= 0: return 0.0\n",
    "    ent = 0.0\n",
    "    for c in counts.values():\n",
    "        if c > 0:\n",
    "            p = c / n\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def majority_label(counts: Dict[Any, float]) -> Any:\n",
    "    return max(counts.items(), key=lambda kv: kv[1])[0] if counts else None\n",
    "\n",
    "def _norm_ppf(p: float) -> float:\n",
    "    # Acklam approximation of inverse normal CDF\n",
    "    a = [-3.969683028665376e+01,  2.209460984245205e+02, -2.759285104469687e+02,\n",
    "          1.383577518672690e+02, -3.066479806614716e+01,  2.506628277459239e+00]\n",
    "    b = [-5.447609879822406e+01,  1.615858368580409e+02, -1.556989798598866e+02,\n",
    "          6.680131188771972e+01, -1.328068155288572e+01]\n",
    "    c = [-7.784894002430293e-03, -3.223964580411365e-01, -2.400758277161838e+00,\n",
    "         -2.549732539343734e+00,  4.374664141464968e+00,  2.938163982698783e+00]\n",
    "    d = [ 7.784695709041462e-03,  3.224671290700398e-01,  2.445134137142996e+00,\n",
    "          3.754408661907416e+00]\n",
    "    plow, phigh = 0.02425, 1 - 0.02425\n",
    "\n",
    "    if p < plow:\n",
    "        q = math.sqrt(-2 * math.log(p))\n",
    "        num = (((((c[0]*q + c[1])*q + c[2])*q + c[3])*q + c[4])*q + c[5])\n",
    "        den = ((((d[0]*q + d[1])*q + d[2])*q + d[3])*q + 1.0)\n",
    "        return num / den\n",
    "\n",
    "    if p > phigh:\n",
    "        q = math.sqrt(-2 * math.log(1 - p))\n",
    "        num = (((((c[0]*q + c[1])*q + c[2])*q + c[3])*q + c[4])*q + c[5])\n",
    "        den = ((((d[0]*q + d[1])*q + d[2])*q + d[3])*q + 1.0)\n",
    "        return - (num / den)\n",
    "\n",
    "    q = p - 0.5\n",
    "    r = q * q\n",
    "    num = (((((a[0]*r + a[1])*r + a[2])*r + a[3])*r + a[4])*r + a[5]) * q\n",
    "    den = (((((b[0]*r + b[1])*r + b[2])*r + b[3])*r + b[4]) * r + 1.0)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def pessimistic_error_upper(e: float, n: float, cf: float) -> float:\n",
    "    if n <= 0: return 0.0\n",
    "    f = e / n\n",
    "    z = _norm_ppf(1 - cf)   # cf=0.25 -> z‚âà0.674\n",
    "    denom = 1 + (z*z)/n\n",
    "    centre = f + (z*z)/(2*n)\n",
    "    adj = z * math.sqrt((f*(1-f) + (z*z)/(4*n))/n)\n",
    "    return (centre + adj)/denom\n",
    "\n",
    "# ---------- tree node\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    is_leaf: bool\n",
    "    prediction: Any\n",
    "    depth: int\n",
    "    n_samples: int\n",
    "    class_counts: Dict[Any, float]\n",
    "    # split info\n",
    "    feature: Optional[str] = None\n",
    "    threshold: Optional[float] = None          # numeric\n",
    "    branches: Optional[Dict[Any, \"Node\"]] = None  # categorical\n",
    "    left: Optional[\"Node\"] = None              # numeric\n",
    "    right: Optional[\"Node\"] = None             # numeric\n",
    "    default_child: Optional[Any] = None        # categorical unseen routing\n",
    "    gain_ratio: float = 0.0\n",
    "    # bookkeeping for raising\n",
    "    idx: Optional[np.ndarray] = None           # training indices that reach this node\n",
    "\n",
    "# ---------- main C5.0\n",
    "\n",
    "class C50DecisionTree:\n",
    "    def __init__(self,\n",
    "                 max_depth: Optional[int]=None,\n",
    "                 max_leaf_nodes: Optional[int]=None,\n",
    "                 min_samples_split: int=20,\n",
    "                 min_samples_leaf: int=10,\n",
    "                 min_gain_ratio: float=0.0,\n",
    "                 cf: float=0.25,\n",
    "                 subtree_raising: bool=True,\n",
    "                 class_weight: Optional[Dict[Any, float]]=None,\n",
    "                 random_state: int=42,\n",
    "                 tie_eps: float=1e-12,\n",
    "                 viz_max_depth: Optional[int]=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_gain_ratio = min_gain_ratio\n",
    "        self.cf = cf\n",
    "        self.subtree_raising = subtree_raising\n",
    "        self.class_weight = class_weight or {}\n",
    "        self.random_state = random_state\n",
    "        self.tie_eps = tie_eps\n",
    "        self.viz_max_depth = viz_max_depth\n",
    "\n",
    "        self._tree_: Optional[Node] = None\n",
    "        self._leaf_count = 0\n",
    "        self._feature_types: Dict[str, str] = {}\n",
    "        self._num_median: Dict[str, float] = {}\n",
    "        self._cat_unknown_token = \"Unknown\"\n",
    "\n",
    "        # keep training data for raising evaluation\n",
    "        self._X: Optional[pd.DataFrame] = None\n",
    "        self._y: Optional[np.ndarray] = None\n",
    "        self._w: Optional[np.ndarray] = None\n",
    "\n",
    "    # ---- preprocessing\n",
    "\n",
    "    def _infer_types(self, df: pd.DataFrame):\n",
    "        for c in df.columns:\n",
    "            self._feature_types[c] = 'numeric' if pd.api.types.is_numeric_dtype(df[c]) else 'categorical'\n",
    "\n",
    "    def _fit_imputers(self, X: pd.DataFrame):\n",
    "        for c, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                self._num_median[c] = float(pd.to_numeric(X[c], errors='coerce').median())\n",
    "\n",
    "    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X2 = X.copy()\n",
    "        for c, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                X2[c] = pd.to_numeric(X2[c], errors='coerce').fillna(self._num_median[c])\n",
    "            else:\n",
    "                X2[c] = X2[c].astype('object').where(X2[c].notna(), self._cat_unknown_token)\n",
    "        return X2\n",
    "\n",
    "    # ---- gain-ratio split search\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: np.ndarray, w: np.ndarray) -> Tuple[float, dict]:\n",
    "        n = len(y)\n",
    "        # weighted parent counts\n",
    "        parent_counts = Counter()\n",
    "        for yi, wi in zip(y, w): parent_counts[yi] += wi\n",
    "        parent_entropy = entropy_from_counts(parent_counts)\n",
    "\n",
    "        best_gr, best_spec = -1.0, None\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        for col, t in self._feature_types.items():\n",
    "            if t == 'numeric':\n",
    "                xs = X[col].values\n",
    "                order = np.argsort(xs)\n",
    "                xs_sorted, y_sorted, w_sorted = xs[order], y[order], w[order]\n",
    "                uniq = np.unique(xs_sorted)\n",
    "                if len(uniq) <= 1: continue\n",
    "                thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "                left_counts = Counter(); left_w = 0.0\n",
    "                total_counts = Counter()\n",
    "                total_w = float(w_sorted.sum())\n",
    "                for yi, wi in zip(y_sorted, w_sorted): total_counts[yi] += wi\n",
    "\n",
    "                ptr = 0\n",
    "                for thr in thresholds:\n",
    "                    while ptr < n and xs_sorted[ptr] <= thr:\n",
    "                        left_counts[y_sorted[ptr]] += w_sorted[ptr]\n",
    "                        left_w += w_sorted[ptr]\n",
    "                        ptr += 1\n",
    "                    nL, nR = left_w, total_w - left_w\n",
    "                    if nL < self.min_samples_leaf or nR < self.min_samples_leaf:\n",
    "                        continue\n",
    "                    right_counts = {k: total_counts[k] - left_counts[k] for k in total_counts}\n",
    "                    gain = parent_entropy \\\n",
    "                        - (nL/total_w)*entropy_from_counts(left_counts) \\\n",
    "                        - (nR/total_w)*entropy_from_counts(right_counts)\n",
    "                    split_info = 0.0\n",
    "                    for m in (nL, nR):\n",
    "                        p = m/total_w\n",
    "                        if p > 0: split_info -= p*math.log2(p)\n",
    "                    if split_info <= 1e-12: continue\n",
    "                    gr = gain / split_info\n",
    "                    # tie-break slightly random within eps\n",
    "                    if gr > best_gr + self.tie_eps or (abs(gr - best_gr) <= self.tie_eps and rng.rand() < 0.5):\n",
    "                        best_gr, best_spec = gr, dict(kind='numeric', feature=col, threshold=float(thr))\n",
    "            else:\n",
    "                groups = defaultdict(list)\n",
    "                for i, v in enumerate(X[col].values): groups[v].append(i)\n",
    "                if len(groups) <= 1: continue\n",
    "\n",
    "                valid = True\n",
    "                child_entropy = 0.0\n",
    "                total_w = float(w.sum())\n",
    "                for idxs in groups.values():\n",
    "                    wk = float(w[idxs].sum())\n",
    "                    if wk < self.min_samples_leaf: valid = False; break\n",
    "                    ck = Counter()\n",
    "                    for ii in idxs: ck[y[ii]] += w[ii]\n",
    "                    child_entropy += (wk/total_w)*entropy_from_counts(ck)\n",
    "                if not valid: continue\n",
    "                gain = parent_entropy - child_entropy\n",
    "                split_info = entropy_from_counts(Counter({k: float(w[idxs].sum()) for k, idxs in groups.items()}))\n",
    "                if split_info <= 1e-12: continue\n",
    "                gr = gain / split_info\n",
    "                if gr > best_gr + self.tie_eps or (abs(gr - best_gr) <= self.tie_eps and rng.rand() < 0.5):\n",
    "                    best_gr = gr\n",
    "                    best_spec = dict(kind='categorical', feature=col,\n",
    "                                     groups={k: np.asarray(v, dtype=int) for k, v in groups.items()})\n",
    "        return best_gr, best_spec\n",
    "\n",
    "    # ---- build\n",
    "\n",
    "    def _weighted_counts(self, y, w) -> Dict[Any, float]:\n",
    "        c = Counter()\n",
    "        for yi, wi in zip(y, w):\n",
    "            c[yi] += wi * (self.class_weight.get(yi, 1.0))\n",
    "        return c\n",
    "\n",
    "    def _build(self, X: pd.DataFrame, y: np.ndarray, w: np.ndarray, depth: int, idx: np.ndarray) -> Node:\n",
    "        counts = self._weighted_counts(y, w)\n",
    "        pred = majority_label(counts)\n",
    "        total_w = float(w.sum())\n",
    "\n",
    "        # stop\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (self.max_leaf_nodes is not None and self._leaf_count >= self.max_leaf_nodes) or \\\n",
    "           total_w < self.min_samples_split or \\\n",
    "           len(counts) == 1:\n",
    "            self._leaf_count += 1\n",
    "            return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "\n",
    "        best_gr, spec = self._best_split(X, y, w)\n",
    "        if spec is None or best_gr <= self.min_gain_ratio:\n",
    "            self._leaf_count += 1\n",
    "            return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "\n",
    "        # split\n",
    "        if spec['kind'] == 'numeric':\n",
    "            f, thr = spec['feature'], spec['threshold']\n",
    "            mask = (X[f].values <= thr)\n",
    "            XL, yL, wL, idxL = X[mask], y[mask], w[mask], idx[mask]\n",
    "            XR, yR, wR, idxR = X[~mask], y[~mask], w[~mask], idx[~mask]\n",
    "            if float(wL.sum()) < self.min_samples_leaf or float(wR.sum()) < self.min_samples_leaf:\n",
    "                self._leaf_count += 1\n",
    "                return Node(True, pred, depth, int(total_w), dict(counts), idx=idx)\n",
    "            left  = self._build(XL, yL, wL, depth+1, idxL)\n",
    "            right = self._build(XR, yR, wR, depth+1, idxR)\n",
    "            node = Node(False, pred, depth, int(total_w), dict(counts),\n",
    "                        feature=f, threshold=thr, left=left, right=right,\n",
    "                        gain_ratio=best_gr, idx=idx)\n",
    "        else:\n",
    "            f, groups = spec['feature'], spec['groups']\n",
    "            branches = {}\n",
    "            # default branch = largest weight child\n",
    "            largest_k = None; largest_w = -1\n",
    "            for k, id_arr in groups.items():\n",
    "                Xi, yi, wi = X.iloc[id_arr], y[id_arr], w[id_arr]\n",
    "                wsum = float(wi.sum())\n",
    "                if wsum < self.min_samples_leaf: continue\n",
    "                branches[k] = self._build(Xi, yi, wi, depth+1, idx[id_arr])\n",
    "                if wsum > largest_w: largest_w, largest_k = wsum, k\n",
    "            node = Node(False, pred, depth, int(total_w), dict(counts),\n",
    "                        feature=f, branches=branches, default_child=largest_k,\n",
    "                        gain_ratio=best_gr, idx=idx)\n",
    "        return node\n",
    "\n",
    "    # ---- pruning + raising\n",
    "\n",
    "    def _subtree_empirical_error(self, node: Node, idx: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"Return (errors, total_w) evaluated on training subset idx with weights/self.class_weight.\"\"\"\n",
    "        if len(idx) == 0: return 0.0, 0.0\n",
    "        Xs = self._X.iloc[idx]\n",
    "        ys = self._y[idx]\n",
    "        ws = self._w[idx]\n",
    "        # predictions\n",
    "        preds = self._predict_batch_rows(Xs, node)\n",
    "        err = 0.0; tot = 0.0\n",
    "        for y_true, y_pred, wi in zip(ys, preds, ws):\n",
    "            w_eff = wi * self.class_weight.get(y_true, 1.0)\n",
    "            tot += w_eff\n",
    "            if y_true != y_pred: err += w_eff\n",
    "        return err, tot\n",
    "\n",
    "    def _prune(self, node: Node) -> Tuple[float, float]:\n",
    "        if node.is_leaf:\n",
    "            e = node.n_samples - node.class_counts.get(node.prediction, 0.0)\n",
    "            return e, float(node.n_samples)\n",
    "\n",
    "        # post-order\n",
    "        if node.branches is not None:\n",
    "            child_err = 0.0; child_w = 0.0\n",
    "            for ch in node.branches.values():\n",
    "                e, w = self._prune(ch)\n",
    "                child_err += e; child_w += w\n",
    "        else:\n",
    "            eL, wL = self._prune(node.left)\n",
    "            eR, wR = self._prune(node.right)\n",
    "            child_err, child_w = eL+eR, wL+wR\n",
    "\n",
    "        # leaf error if collapsed\n",
    "        leaf_err = node.n_samples - node.class_counts.get(node.prediction, 0.0)\n",
    "\n",
    "        child_rate = pessimistic_error_upper(child_err, child_w, self.cf)\n",
    "        leaf_rate  = pessimistic_error_upper(leaf_err,  float(node.n_samples), self.cf)\n",
    "\n",
    "        if leaf_rate <= child_rate + 1e-12:\n",
    "            node.is_leaf = True\n",
    "            node.feature = node.threshold = None\n",
    "            node.branches = None; node.left = node.right = None\n",
    "            return leaf_err, float(node.n_samples)\n",
    "        return child_err, child_w\n",
    "\n",
    "    def _assign_indices(self, node: Node, idx: np.ndarray):\n",
    "        \"\"\"Push parent idx down the subtree to refresh children's idx after raising.\"\"\"\n",
    "        node.idx = idx\n",
    "        if node.is_leaf: return\n",
    "        Xs = self._X.iloc[idx]\n",
    "        if node.branches is not None:\n",
    "            # categorical\n",
    "            buckets = defaultdict(list)\n",
    "            col = node.feature\n",
    "            for i, v in zip(idx, Xs[col].values):\n",
    "                buckets[v].append(i)\n",
    "            for k, ch in node.branches.items():\n",
    "                self._assign_indices(ch, np.array(buckets.get(k, []), dtype=int))\n",
    "        else:\n",
    "            # numeric\n",
    "            col, thr = node.feature, node.threshold\n",
    "            mask = (Xs[col].values <= thr)\n",
    "            self._assign_indices(node.left,  idx[mask])\n",
    "            self._assign_indices(node.right, idx[~mask])\n",
    "\n",
    "    def _try_raising_here(self, node: Node):\n",
    "        \"\"\"Try subtree raising: replace 'node' with one of its children if pessimistic error doesn't get worse.\"\"\"\n",
    "        if node.is_leaf: return\n",
    "        # evaluate current subtree on node.idx\n",
    "        cur_err, cur_w = self._subtree_empirical_error(node, node.idx)\n",
    "        cur_rate = pessimistic_error_upper(cur_err, cur_w, self.cf)\n",
    "\n",
    "        candidates = []\n",
    "        if node.branches is not None:\n",
    "            candidates = list(node.branches.values())\n",
    "        else:\n",
    "            candidates = [node.left, node.right]\n",
    "\n",
    "        for child in candidates:\n",
    "            ch_err, ch_w = self._subtree_empirical_error(child, node.idx)\n",
    "            ch_rate = pessimistic_error_upper(ch_err, ch_w, self.cf)\n",
    "            if ch_rate <= cur_rate - 1e-12:   # strictly better\n",
    "                # raise: copy child's structure into node\n",
    "                # (deep copy to avoid aliasing other references)\n",
    "                clone = copy.deepcopy(child)\n",
    "                node.is_leaf = clone.is_leaf\n",
    "                node.prediction = clone.prediction\n",
    "                node.depth = node.depth  # keep same\n",
    "                node.n_samples = node.n_samples\n",
    "                node.class_counts = node.class_counts\n",
    "                node.feature = clone.feature\n",
    "                node.threshold = clone.threshold\n",
    "                node.branches = clone.branches\n",
    "                node.left = clone.left\n",
    "                node.right = clone.right\n",
    "                node.default_child = clone.default_child\n",
    "                node.gain_ratio = clone.gain_ratio\n",
    "                # re-distribute indices to refreshed structure\n",
    "                self._assign_indices(node, node.idx)\n",
    "                # after one successful raising,ÂèØ‰ª•ÂÜçÂòóË©¶ÈÄ£ÈéñÊèêÂçá\n",
    "                return self._try_raising_here(node)\n",
    "\n",
    "    def _raising(self, node: Node):\n",
    "        if node.is_leaf: return\n",
    "        # post-order\n",
    "        if node.branches is not None:\n",
    "            for ch in node.branches.values():\n",
    "                self._raising(ch)\n",
    "        else:\n",
    "            self._raising(node.left)\n",
    "            self._raising(node.right)\n",
    "        # then try raising at this node\n",
    "        self._try_raising_here(node)\n",
    "\n",
    "    # ---- API\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, sample_weight: Optional[np.ndarray]=None):\n",
    "        self._X = X.copy()\n",
    "        yy = y.values if isinstance(y, pd.Series) else np.asarray(y)\n",
    "        self._y = yy\n",
    "        self._w = sample_weight if sample_weight is not None else np.ones(len(yy), dtype=float)\n",
    "\n",
    "        self._infer_types(self._X)\n",
    "        self._fit_imputers(self._X)\n",
    "        X2 = self._transform(self._X)\n",
    "\n",
    "        self._leaf_count = 0\n",
    "        idx_all = np.arange(len(yy), dtype=int)\n",
    "        self._tree_ = self._build(X2, yy, self._w, depth=0, idx=idx_all)\n",
    "        # pruning\n",
    "        self._prune(self._tree_)\n",
    "        # raising\n",
    "        if self.subtree_raising:\n",
    "            self._assign_indices(self._tree_, idx_all)\n",
    "            self._raising(self._tree_)\n",
    "        return self\n",
    "\n",
    "    def _predict_row(self, row: pd.Series, node: Node):\n",
    "        while not node.is_leaf:\n",
    "            if node.branches is not None:\n",
    "                v = row[node.feature]\n",
    "                child = node.branches.get(v)\n",
    "                if child is None:\n",
    "                    child = node.branches[node.default_child]\n",
    "                node = child\n",
    "            else:\n",
    "                node = node.left if row[node.feature] <= node.threshold else node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _predict_batch_rows(self, X: pd.DataFrame, node: Optional[Node]=None):\n",
    "        if node is None: node = self._tree_\n",
    "        return [self._predict_row(X.iloc[i], node) for i in range(len(X))]\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        X2 = self._transform(X)\n",
    "        return np.array(self._predict_batch_rows(X2))\n",
    "\n",
    "    # ---- pretty print\n",
    "\n",
    "    def print_tree(self, node: Optional[Node]=None, depth: int=0, max_depth: Optional[int]=None):\n",
    "        if node is None: node = self._tree_\n",
    "        if node is None:\n",
    "            print(\"(empty)\"); return\n",
    "        if max_depth is None: max_depth = self.viz_max_depth\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf or depth >= max_depth:\n",
    "            print(f\"{indent}Leaf[n={node.n_samples}] -> {node.prediction}  counts={node.class_counts}\")\n",
    "            return\n",
    "        if node.branches is not None:\n",
    "            print(f\"{indent}Split @[#{node.feature}]  GR={node.gain_ratio:.4f}  n={node.n_samples}\")\n",
    "            for k, ch in node.branches.items():\n",
    "                print(f\"{indent} ‚îú‚îÄ {k}:\")\n",
    "                self.print_tree(ch, depth+1, max_depth)\n",
    "        else:\n",
    "            print(f\"{indent}Split @[{node.feature} <= {node.threshold:.4f}]  GR={node.gain_ratio:.4f}  n={node.n_samples}\")\n",
    "            print(f\"{indent} ‚îú‚îÄ True:\");  self.print_tree(node.left,  depth+1, max_depth)\n",
    "            print(f\"{indent} ‚îî‚îÄ False:\"); self.print_tree(node.right, depth+1, max_depth)\n",
    "\n",
    "# =========================\n",
    "# DatasetÔºà‰Ω†ÁöÑÊ®ôÁ±§Ê¨Ñ‰ΩçÔºöclassÔºâ\n",
    "# =========================\n",
    "# ======================\n",
    "# ÂèÉÊï∏Ôºà‰Ω†ÊåáÂÆöÁöÑÊ®£ÂºèÔºâ\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH = \"adult_data_no_duplicates.csv\"\n",
    "TEST_PATH  = \"adult_test_no_duplicates.csv\"\n",
    "\n",
    "MAX_DEPTH         = 10\n",
    "MAX_LEAF_NODES    = 64\n",
    "MIN_SAMPLES_SPLIT = 20\n",
    "MIN_SAMPLES_LEAF  = 10          # ‚Üê ‰Ω†Ê≤íÂàóÂà∞Ôºå‰ΩÜÊ±∫Á≠ñÊ®πÂª∫Ë≠∞‰∏Ä‰ΩµË®≠\n",
    "MIN_GAIN          = 1e-4        # C4.5/C5.0ÔºöË¶ñÁÇ∫ min_gain_ratio ÈñÄÊ™ª\n",
    "CF                = 0.25        # ÊÇ≤ËßÄË™§Â∑Æ‰øÆÂâ™ÁöÑ‰ø°Ë≥¥‰øÇÊï∏ (C4.5/C5.0)\n",
    "USE_RAISING       = True        # C5.0 Â≠êÊ®πÊèêÂçá\n",
    "\n",
    "# ======================\n",
    "# ËÆÄË≥áÊñôÔºàÊ®ôÁ±§Ê¨Ñ‰Ωç=classÔºâ\n",
    "# ======================\n",
    "# ====== Â∞çÈΩäÊ®ôÁ±§Ê¨ÑÔºàÊîæÂú®ËÆÄÂÆå CSV ‰πãÂæåÔºâ ======\n",
    "\n",
    "def load_adult(train_path=TRAIN_PATH, test_path=TEST_PATH,\n",
    "               target_candidates=(\"class\",\"income\",\"label\",\"target\",\"y\")):\n",
    "    # 1) ËÆÄÊ™î\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    # 2) Ê¨ÑÂêçÊ≠£Ë¶èÂåñ\n",
    "    def _norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df.columns = [str(c).replace(\"\\ufeff\",\"\").strip().lower() for c in df.columns]\n",
    "        return df\n",
    "    train_df = _norm_cols(train_df)\n",
    "    test_df  = _norm_cols(test_df)\n",
    "\n",
    "    # 3) Áî±Ë®ìÁ∑¥ÈõÜÊ±∫ÂÆöÁõÆÊ®ôÊ¨ÑÔºõÊ∏¨Ë©¶ÈõÜÊîπÂêçÂ∞çÈΩä\n",
    "    target_col = next((c for c in train_df.columns if c in target_candidates),\n",
    "                      train_df.columns[-1])\n",
    "    if target_col not in test_df.columns:\n",
    "        test_df = test_df.rename(columns={test_df.columns[-1]: target_col})\n",
    "\n",
    "    # 4) Ê®ôÁ±§Ê≠£Ë¶èÂåñÔºàÁßªÈô§Âè•Èªû„ÄÅÁµ±‰∏ÄÂ§ßÂ∞èÂØ´Ôºâ\n",
    "    def _norm_labels(s: pd.Series) -> pd.Series:\n",
    "        if s.dtype == object or s.dtype.name == \"category\":\n",
    "            return (s.astype(str).str.strip()\n",
    "                    .str.replace(r\"\\.$\",\"\", regex=True)\n",
    "                    .str.replace(\">50k\", \">50K\", case=False)\n",
    "                    .str.replace(\"<=50k\",\"<=50K\", case=False))\n",
    "        return s\n",
    "\n",
    "    X_train = train_df.drop(columns=[target_col]).replace(\"?\", np.nan)\n",
    "    y_train = _norm_labels(train_df[target_col])\n",
    "    X_test  = test_df.drop(columns=[target_col]).replace(\"?\", np.nan)\n",
    "    y_test  = _norm_labels(test_df[target_col])\n",
    "\n",
    "    print(\"‰ΩøÁî®ÁöÑÁõÆÊ®ôÊ¨Ñ‰Ωç:\", target_col)\n",
    "    print(\"train ÊúÄÂæå5ÂÄãÊ¨Ñ‰Ωç:\", train_df.columns[-5:].tolist())\n",
    "    print(\"test  ÊúÄÂæå5ÂÄãÊ¨Ñ‰Ωç:\",  test_df.columns[-5:].tolist())\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# ‰∏ÄÈçµÂèñÂæóË≥áÊñô\n",
    "X_train, y_train, X_test, y_test = load_adult()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# C5.0ÔºàÁî®‰Ω†ÊåáÂÆöÁöÑÂèÉÊï∏Ôºâ\n",
    "# ÈúÄÂÖàÂ∑≤ÂÆöÁæ©Â•Ω C50DecisionTree È°ûÂà•ÔºàÂâçÈù¢ÊàëÁµ¶ÈÅéÈÇ£‰ªΩÔºâ\n",
    "# ======================\n",
    "c50 = C50DecisionTree(\n",
    "    max_depth=MAX_DEPTH,\n",
    "    max_leaf_nodes=MAX_LEAF_NODES,\n",
    "    min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "    min_samples_leaf=MIN_SAMPLES_LEAF,\n",
    "    min_gain_ratio=MIN_GAIN,    # ‚Üê Áî®‰Ω†ÁöÑ MIN_GAIN Áï∂ gain-ratio ÈñÄÊ™ª\n",
    "    cf=CF,\n",
    "    subtree_raising=USE_RAISING,\n",
    "    viz_max_depth=3\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "pred_tr = c50.predict(X_train)\n",
    "pred_te = c50.predict(X_test)\n",
    "print(\"\\n=== C5.0Ôºàwith raising={}Ôºâ ===\".format(USE_RAISING))\n",
    "print(\"Train Acc:\", accuracy_score(y_train, pred_tr))\n",
    "print(\"Test  Acc:\", accuracy_score(y_test,  pred_te))\n",
    "print(classification_report(y_test, pred_te))\n",
    "c50.print_tree(max_depth=3)  # Âè™Êà™ÂúñÂà∞Ê®πÈ´ò=3ÔºåË®ìÁ∑¥‰ªç‰æù MAX_DEPTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9056fc56-6fa5-41b8-9c5e-2adbbe6c38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñºÔ∏è Áî¢Áîü C5.0 Ê±∫Á≠ñÊ®πË¶ñË¶∫Âåñ (depth<=3, leaf<=64, black & white)...\n",
      "‚úÖ Â∑≤‰ΩøÁî® Graphviz Ëº∏Âá∫ Tree_C50_Custom.pngÔºàÊ∑±Â∫¶=3, ÁØÄÈªû‚â§64, ÈªëÁôΩÈ¢®Ê†ºÔºâ\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Step: Áπ™Ë£Ω C5.0 Ê±∫Á≠ñÊ®πÂúñÔºàËá™ÂãïÂÅµÊ∏¨ Graphviz Êàñ MatplotlibÔºâ\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz, plot_tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Áµ±‰∏ÄÁπ™ÂúñÂèÉÊï∏\n",
    "MAX_DEPTH_VIS = 3\n",
    "MAX_LEAF_VIS  = 64\n",
    "\n",
    "try:\n",
    "    import graphviz\n",
    "    HAS_GRAPHVIZ = True\n",
    "except ImportError:\n",
    "    HAS_GRAPHVIZ = False\n",
    "    print(\"Êú™ÂÅµÊ∏¨Âà∞ GraphvizÔºåÂ∞áËá™ÂãïÊîπÁî® Matplotlib Ëº∏Âá∫ C5.0 Ê±∫Á≠ñÊ®π„ÄÇ\")\n",
    "\n",
    "# --- Step 1: Â∞á C5.0 ÁöÑË¶èÂâáÊ®πÈáçÊñ∞Êì¨ÂêàÊàê sklearn.DecisionTreeClassifier ‰ª•Âà©Ë¶ñË¶∫Âåñ ---\n",
    "# ÔºàÂõ† C5.0 ÊòØÊâãÂàªÁâàÔºåGraphviz ‰∏çÊîØÊè¥Ëá™Ë®ÇÁµêÊßãÔºâ\n",
    "# ÈÄôË£°ÁöÑÁõÆÁöÑÂÉÖÁÇ∫Áï´ÂúñÔºåÈùûÈáçÊñ∞Ë®ìÁ∑¥Ê®°Âûã\n",
    "print(\"\\nÁî¢Áîü C5.0 Ê±∫Á≠ñÊ®πË¶ñË¶∫Âåñ (depth<=3, leaf<=64, black & white)...\")\n",
    "\n",
    "# Â∞áË®ìÁ∑¥Ë≥áÊñôËΩâÁÇ∫ÂèØÊï∏ÂÄºÂåñÔºàLabelEncoderÔºâ\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_enc = X_train.copy()\n",
    "for c in X_enc.columns:\n",
    "    if X_enc[c].dtype == 'object':\n",
    "        X_enc[c] = LabelEncoder().fit_transform(X_enc[c].astype(str))\n",
    "y_enc = LabelEncoder().fit_transform(y_train)\n",
    "\n",
    "# Áî® sklearn CART Ê®°Êì¨ C5.0 ÁöÑÁµêÊßãÔºåÂè™‰ΩúÁπ™ÂúñÁî®ÈÄî\n",
    "clf_c50_vis = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",     # Ê®°Êì¨ C5.0 ÁöÑË≥áË®äÂ¢ûÁõäÁéáÈÇèËºØ\n",
    "    max_depth=MAX_DEPTH_VIS,\n",
    "    max_leaf_nodes=MAX_LEAF_VIS,\n",
    "    random_state=42\n",
    ")\n",
    "clf_c50_vis.fit(X_enc, y_enc)\n",
    "\n",
    "# --- Step 2: Ëº∏Âá∫ ---\n",
    "if HAS_GRAPHVIZ:\n",
    "    try:\n",
    "        dot_c50 = export_graphviz(\n",
    "            clf_c50_vis,\n",
    "            out_file=None,\n",
    "            feature_names=X_enc.columns,\n",
    "            class_names=np.unique(y_train).astype(str),\n",
    "            filled=False,   # ÈªëÁôΩ\n",
    "            rounded=True,\n",
    "            max_depth=MAX_DEPTH_VIS\n",
    "        )\n",
    "        graphviz.Source(dot_c50).render(\"Tree_C50_Custom\", format='png', cleanup=True)\n",
    "        print(\"Â∑≤‰ΩøÁî® Graphviz Ëº∏Âá∫ Tree_C50_Custom.pngÔºàÊ∑±Â∫¶=3, ÁØÄÈªû‚â§64, ÈªëÁôΩÈ¢®Ê†ºÔºâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"Graphviz Áπ™ÂúñÂ§±Êïó ({e})ÔºåÊîπÁî® Matplotlib„ÄÇ\")\n",
    "        HAS_GRAPHVIZ = False\n",
    "\n",
    "# --- Step 3: Matplotlib ÂÇôÊè¥ ---\n",
    "if not HAS_GRAPHVIZ:\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plot_tree(\n",
    "        clf_c50_vis,\n",
    "        feature_names=X_enc.columns,\n",
    "        class_names=np.unique(y_train).astype(str),\n",
    "        filled=False,  # ÈªëÁôΩÈ¢®Ê†º\n",
    "        rounded=True,\n",
    "        max_depth=MAX_DEPTH_VIS\n",
    "    )\n",
    "    plt.title(\"C5.0 Decision Tree (Depth=3, Leaf<=64, Black & White)\", fontsize=14)\n",
    "    plt.savefig(\"Tree_C50_Custom.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Â∑≤‰ΩøÁî® Matplotlib Ëº∏Âá∫ Tree_C50_Custom.pngÔºàÊ∑±Â∫¶=3, ÁØÄÈªû‚â§64, ÈªëÁôΩÈ¢®Ê†ºÔºâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99d2d1-eccb-4e74-92cd-fe6d04178194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
