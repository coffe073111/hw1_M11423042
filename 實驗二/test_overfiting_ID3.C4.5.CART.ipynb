{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfeadb88-b719-404b-97d5-54545d267ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 偵測到 test 標籤欄位：calss -> 重新命名為 class\n",
      "[Info] class_names = ['<=50K', '>50K']  (0 -> <=50K, 1 -> >50K)\n",
      "\n",
      "=== 自動調參：ID3 ===\n",
      "[ID3] depth=0 feat=relationship (multiway) metric=0.165337 n=32537\n",
      "[ID3] depth=1 feat=education (multiway) metric=0.132089 n=13187\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.052372 n=313\n",
      "[ID3] depth=2 feat=capital-gain thr=4164.000000 metric=0.117210 n=311\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.161556 n=103\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.179383 n=70\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.104546 n=149\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.046970 n=332\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.037081 n=202\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080987 n=380\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.074866 n=596\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.076456 n=2433\n",
      "[ID3] depth=2 feat=capital-gain thr=5054.500000 metric=0.047987 n=265\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.066894 n=4276\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.063746 n=886\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.077833 n=385\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080568 n=2472\n",
      "[ID3] depth=1 feat=capital-gain thr=8296.000000 metric=0.093364 n=8292\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.046582 n=8065\n",
      "[ID3] depth=2 feat=workclass (multiway) metric=0.015269 n=227\n",
      "[ID3] depth=1 feat=capital-gain thr=4243.500000 metric=0.038171 n=981\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.031909 n=967\n",
      "[ID3] depth=1 feat=capital-gain thr=4718.500000 metric=0.021392 n=5064\n",
      "[ID3] depth=2 feat=marital-status (multiway) metric=0.009593 n=5033\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.255562 n=31\n",
      "[ID3] depth=1 feat=capital-gain thr=7139.500000 metric=0.056824 n=3445\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.034458 n=3389\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.197578 n=56\n",
      "[ID3] depth=1 feat=occupation (multiway) metric=0.130233 n=1568\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.072815 n=353\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.209197 n=25\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.081315 n=240\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.195749 n=84\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.087349 n=183\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.072978 n=307\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.108315 n=130\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.363569 n=49\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.136361 n=133\n",
      "[ID3][step 1] Train=0.8947 Test=0.8341 Δ=0.0607  params={'max_depth': 10, 'min_samples_split': 20, 'min_gain': 0.0001}\n",
      "[ID3] depth=0 feat=relationship (multiway) metric=0.165337 n=32537\n",
      "[ID3] depth=1 feat=education (multiway) metric=0.132089 n=13187\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.052372 n=313\n",
      "[ID3] depth=2 feat=capital-gain thr=4164.000000 metric=0.117210 n=311\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.161556 n=103\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.179383 n=70\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.104546 n=149\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.046970 n=332\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.037081 n=202\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080987 n=380\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.074866 n=596\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.076456 n=2433\n",
      "[ID3] depth=2 feat=capital-gain thr=5054.500000 metric=0.047987 n=265\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.066894 n=4276\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.063746 n=886\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.077833 n=385\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080568 n=2472\n",
      "[ID3] depth=1 feat=capital-gain thr=8296.000000 metric=0.093364 n=8292\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.046582 n=8065\n",
      "[ID3] depth=2 feat=workclass (multiway) metric=0.015269 n=227\n",
      "[ID3] depth=1 feat=capital-gain thr=4243.500000 metric=0.038171 n=981\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.031909 n=967\n",
      "[ID3] depth=1 feat=capital-gain thr=4718.500000 metric=0.021392 n=5064\n",
      "[ID3] depth=2 feat=marital-status (multiway) metric=0.009593 n=5033\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.255562 n=31\n",
      "[ID3] depth=1 feat=capital-gain thr=7139.500000 metric=0.056824 n=3445\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.034458 n=3389\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.197578 n=56\n",
      "[ID3] depth=1 feat=occupation (multiway) metric=0.130233 n=1568\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.072815 n=353\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.209197 n=25\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.081315 n=240\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.195749 n=84\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.087349 n=183\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.072978 n=307\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.108315 n=130\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.363569 n=49\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.136361 n=133\n",
      "[ID3][step 2] Train=0.8862 Test=0.8386 Δ=0.0476  params={'max_depth': 9, 'min_samples_split': 25, 'min_gain': 0.0002}\n",
      "[ID3] depth=0 feat=relationship (multiway) metric=0.165337 n=32537\n",
      "[ID3] depth=1 feat=education (multiway) metric=0.132089 n=13187\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.052372 n=313\n",
      "[ID3] depth=2 feat=capital-gain thr=4164.000000 metric=0.117210 n=311\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.161556 n=103\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.179383 n=70\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.104546 n=149\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.046970 n=332\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.037081 n=202\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080987 n=380\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.074866 n=596\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.076456 n=2433\n",
      "[ID3] depth=2 feat=capital-gain thr=5054.500000 metric=0.047987 n=265\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.066894 n=4276\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.063746 n=886\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.077833 n=385\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080568 n=2472\n",
      "[ID3] depth=1 feat=capital-gain thr=8296.000000 metric=0.093364 n=8292\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.046582 n=8065\n",
      "[ID3] depth=2 feat=workclass (multiway) metric=0.015269 n=227\n",
      "[ID3] depth=1 feat=capital-gain thr=4243.500000 metric=0.038171 n=981\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.031909 n=967\n",
      "[ID3] depth=1 feat=capital-gain thr=4718.500000 metric=0.021392 n=5064\n",
      "[ID3] depth=2 feat=marital-status (multiway) metric=0.009593 n=5033\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.255562 n=31\n",
      "[ID3] depth=1 feat=capital-gain thr=7139.500000 metric=0.056824 n=3445\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.034458 n=3389\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.197578 n=56\n",
      "[ID3] depth=1 feat=occupation (multiway) metric=0.130233 n=1568\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.072815 n=353\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.081315 n=240\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.195749 n=84\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.087349 n=183\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.072978 n=307\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.108315 n=130\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.363569 n=49\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.136361 n=133\n",
      "[ID3][step 3] Train=0.8793 Test=0.8406 Δ=0.0387  params={'max_depth': 8, 'min_samples_split': 31, 'min_gain': 0.0004}\n",
      "[ID3] depth=0 feat=relationship (multiway) metric=0.165337 n=32537\n",
      "[ID3] depth=1 feat=education (multiway) metric=0.132089 n=13187\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.052372 n=313\n",
      "[ID3] depth=2 feat=capital-gain thr=4164.000000 metric=0.117210 n=311\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.161556 n=103\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.179383 n=70\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.104546 n=149\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.046970 n=332\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.037081 n=202\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080987 n=380\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.074866 n=596\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.076456 n=2433\n",
      "[ID3] depth=2 feat=capital-gain thr=5054.500000 metric=0.047987 n=265\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.066894 n=4276\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.063746 n=886\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.077833 n=385\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080568 n=2472\n",
      "[ID3] depth=1 feat=capital-gain thr=8296.000000 metric=0.093364 n=8292\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.046582 n=8065\n",
      "[ID3] depth=2 feat=workclass (multiway) metric=0.015269 n=227\n",
      "[ID3] depth=1 feat=capital-gain thr=4243.500000 metric=0.038171 n=981\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.031909 n=967\n",
      "[ID3] depth=1 feat=capital-gain thr=4718.500000 metric=0.021392 n=5064\n",
      "[ID3] depth=2 feat=marital-status (multiway) metric=0.009593 n=5033\n",
      "[ID3] depth=1 feat=capital-gain thr=7139.500000 metric=0.056824 n=3445\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.034458 n=3389\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.197578 n=56\n",
      "[ID3] depth=1 feat=occupation (multiway) metric=0.130233 n=1568\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.072815 n=353\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.081315 n=240\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.195749 n=84\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.087349 n=183\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.072978 n=307\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.108315 n=130\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.363569 n=49\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.136361 n=133\n",
      "[ID3][step 4] Train=0.8734 Test=0.8439 Δ=0.0294  params={'max_depth': 7, 'min_samples_split': 38, 'min_gain': 0.0008}\n",
      "[ID3] depth=0 feat=relationship (multiway) metric=0.165337 n=32537\n",
      "[ID3] depth=1 feat=education (multiway) metric=0.132089 n=13187\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.052372 n=313\n",
      "[ID3] depth=2 feat=capital-gain thr=4164.000000 metric=0.117210 n=311\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.161556 n=103\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.179383 n=70\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.104546 n=149\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.046970 n=332\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.037081 n=202\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080987 n=380\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.074866 n=596\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.076456 n=2433\n",
      "[ID3] depth=2 feat=capital-gain thr=5054.500000 metric=0.047987 n=265\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.066894 n=4276\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.063746 n=886\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.077833 n=385\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.080568 n=2472\n",
      "[ID3] depth=1 feat=capital-gain thr=8296.000000 metric=0.093364 n=8292\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.046582 n=8065\n",
      "[ID3] depth=2 feat=workclass (multiway) metric=0.015269 n=227\n",
      "[ID3] depth=1 feat=capital-gain thr=4243.500000 metric=0.038171 n=981\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.031909 n=967\n",
      "[ID3] depth=1 feat=capital-gain thr=4718.500000 metric=0.021392 n=5064\n",
      "[ID3] depth=2 feat=marital-status (multiway) metric=0.009593 n=5033\n",
      "[ID3] depth=1 feat=capital-gain thr=7139.500000 metric=0.056824 n=3445\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.034458 n=3389\n",
      "[ID3] depth=2 feat=occupation (multiway) metric=0.197578 n=56\n",
      "[ID3] depth=1 feat=occupation (multiway) metric=0.130233 n=1568\n",
      "[ID3] depth=2 feat=capital-gain thr=4843.000000 metric=0.072815 n=353\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.081315 n=240\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.195749 n=84\n",
      "[ID3] depth=2 feat=native-country (multiway) metric=0.087349 n=183\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.072978 n=307\n",
      "[ID3] depth=2 feat=capital-gain thr=5095.500000 metric=0.108315 n=130\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.363569 n=49\n",
      "[ID3] depth=2 feat=education (multiway) metric=0.136361 n=133\n",
      "[ID3][step 5] Train=0.8665 Test=0.8452 Δ=0.0213  params={'max_depth': 6, 'min_samples_split': 47, 'min_gain': 0.0016}\n",
      "\n",
      "=== 自動調參：C4.5 ===\n",
      "[C4.5] depth=0 feat=capital-gain thr=7073.500000 metric=0.339996 n=32537\n",
      "[C4.5] depth=1 feat=capital-loss thr=1881.500000 metric=0.146331 n=31138\n",
      "[C4.5] depth=2 feat=education-num thr=14.500000 metric=0.098485 n=30242\n",
      "[C4.5] depth=2 feat=capital-loss thr=1978.500000 metric=0.269680 n=896\n",
      "[C4.5] depth=1 feat=age thr=18.500000 metric=0.569407 n=1399\n",
      "[C4.5] depth=2 feat=education-num thr=1.500000 metric=0.531401 n=1397\n",
      "[C4.5][step 1] Train=0.8331 Test=0.8309 Δ=0.0022  params={'max_depth': 10, 'min_samples_split': 20, 'min_gain': 0.0001}\n",
      "\n",
      "=== 自動調參：CART ===\n",
      "[CART][step 1] Train=0.8636 Test=0.8611 Δ=0.0025  params={'max_depth': 10, 'min_samples_split': 20, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 64, 'ccp_alpha': 2.8173054266424893e-05}\n",
      "\n",
      "=== 輸出樹圖（僅前 3 層） ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.87624 to fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 手刻樹圖片：Tree_ID3.png\n",
      "[OK] 手刻樹圖片：Tree_C45.png\n",
      "[OK] sklearn 樹圖片：Tree_CART.png\n",
      "\n",
      "=== 評估（最終模型）===\n",
      "\n",
      "=== Accuracy (Train / Test) ===\n",
      "Model  Train_Acc  Test_Acc\n",
      "  ID3   0.866460  0.845171\n",
      " C4.5   0.833082  0.830917\n",
      " CART   0.863632  0.861084\n",
      "\n",
      "[ID3] Accuracy = 0.8452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8764    0.9282    0.9015     12430\n",
      "        >50K     0.7130    0.5770    0.6378      3846\n",
      "\n",
      "    accuracy                         0.8452     16276\n",
      "   macro avg     0.7947    0.7526    0.7697     16276\n",
      "weighted avg     0.8378    0.8452    0.8392     16276\n",
      "\n",
      "[OK] 混淆矩陣：CM_ID3.png\n",
      "\n",
      "[C4.5] Accuracy = 0.8309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8235    0.9909    0.8995     12430\n",
      "        >50K     0.9144    0.3138    0.4673      3846\n",
      "\n",
      "    accuracy                         0.8309     16276\n",
      "   macro avg     0.8690    0.6524    0.6834     16276\n",
      "weighted avg     0.8450    0.8309    0.7974     16276\n",
      "\n",
      "[OK] 混淆矩陣：CM_C45.png\n",
      "\n",
      "[CART] Accuracy = 0.8611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8827    0.9435    0.9121     12430\n",
      "        >50K     0.7651    0.5946    0.6692      3846\n",
      "\n",
      "    accuracy                         0.8611     16276\n",
      "   macro avg     0.8239    0.7691    0.7906     16276\n",
      "weighted avg     0.8549    0.8611    0.8547     16276\n",
      "\n",
      "[OK] 混淆矩陣：CM_CART.png\n",
      "\n",
      "=== 完成 ===\n",
      "輸出：Tree_ID3.png / Tree_C45.png / Tree_CART.png / CM_ID3.png / CM_C45.png / CM_CART.png / accuracy_train_test.csv / tune_log.csv\n",
      "[說明] 過擬合門檻 GAP_THRESHOLD = 0.025（可改為 3×SE 的資料驅動版本；見下方參考）\n",
      "[參考] 以 p≈0.8611, n=16276, 3×SE≈0.8133%，資料驅動門檻建議 ≥ 2.00%\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ID3 / C4.5（手刻，多路切） vs CART（sklearn）\n",
    "# - 固定標籤欄位為 class；自動 calss→class；清理標籤尾巴的句點與空白\n",
    "# - graphviz 不在時使用 Matplotlib 後備繪圖\n",
    "# - 加入：過擬合偵測（ΔAcc）＋自動調參（預剪枝/後剪枝）\n",
    "# ===============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
    "\n",
    "# ---------------- 全域參數（初始值；若過擬合會自動微調） ----------------\n",
    "TRAIN_MAX_DEPTH    = 10     # 允許的初始最大深度\n",
    "MIN_SAMPLES_SPLIT  = 20     # 允許再分裂的最小樣本數\n",
    "MIN_GAIN           = 1e-4   # ID3: 資訊增益門檻；C4.5: 增益率門檻；CART: 對應 min_impurity_decrease\n",
    "MAX_LEAF_NODES     = 64     # CART 專用（葉子上限）\n",
    "VIEW_DEPTH         = 3      # 圖片只顯示前 3 層\n",
    "\n",
    "# =========== 嘗試載入 graphviz，失敗就走 Matplotlib ===========\n",
    "try:\n",
    "    from graphviz import Source\n",
    "    _GRAPHVIZ_OK = True\n",
    "except Exception:\n",
    "    _GRAPHVIZ_OK = False\n",
    "    print(\"[Warning] 未偵測到 graphviz，將改用 Matplotlib 後備繪圖。\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# 基本工具\n",
    "# -----------------------------\n",
    "def load_csvs(train_path: str, test_path: str):\n",
    "    return pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "\n",
    "def strip_object_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def standardize_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        cc = str(c).replace(\"\\ufeff\", \"\").strip()  # 去 BOM/空白\n",
    "        cc = cc.replace(\"\\t\",\" \").replace(\"  \",\" \")\n",
    "        cc = cc.replace(\":\", \"\").replace(\".\", \"\").strip()  # 移除常見雜點號\n",
    "        new_cols.append(cc)\n",
    "    df = df.copy()\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def coerce_numeric_columns(df: pd.DataFrame, exclude: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        if df[c].dtype == \"object\":\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().mean() > 0.9:\n",
    "                df[c] = s\n",
    "    return df\n",
    "\n",
    "def basic_impute(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if c == target:\n",
    "            continue\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].fillna(\"Unknown\").replace({\"?\":\"Unknown\",\" ?\":\"Unknown\"})\n",
    "        else:\n",
    "            med = pd.to_numeric(df[c], errors=\"coerce\").median()\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(med)\n",
    "    return df\n",
    "\n",
    "def _clean_label_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"adult 測試檔常見：末尾有小數點、含空白。\"\"\"\n",
    "    return s.astype(str).str.strip().str.replace(\".\", \"\", regex=False)\n",
    "\n",
    "def split_xy(df: pd.DataFrame, target: str):\n",
    "    \"\"\"只切分 X / y（前置清理已完成）\"\"\"\n",
    "    return df.drop(columns=[target]).copy(), df[target].copy()\n",
    "\n",
    "def get_feature_types(X: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"判斷每欄是 numeric 或 categorical，供手刻 ID3/C4.5 使用。\"\"\"\n",
    "    types = []\n",
    "    for c in X.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X[c]):\n",
    "            types.append(\"numeric\")\n",
    "        else:\n",
    "            coer = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            types.append(\"numeric\" if coer.notna().mean() > 0.9 else \"categorical\")\n",
    "    return types\n",
    "\n",
    "def to_numpy_mixed(X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"輸出 object dtype 的 numpy；數值欄轉 float，類別欄保留原值。\"\"\"\n",
    "    arr = X.to_numpy(dtype=object)\n",
    "    for j, c in enumerate(X.columns):\n",
    "        if pd.api.types.is_numeric_dtype(X[c]) or pd.to_numeric(X[c], errors=\"coerce\").notna().mean() > 0.9:\n",
    "            arr[:, j] = pd.to_numeric(X[c], errors=\"coerce\").astype(float).to_numpy()\n",
    "        else:\n",
    "            arr[:, j] = X[c].astype(object).to_numpy()\n",
    "    return arr\n",
    "\n",
    "def make_label_encoder(y_train: pd.Series, y_test: pd.Series) -> Tuple[list, dict, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    產生二元標籤編碼：\n",
    "    - 多數類放在 index 0（混淆矩陣/報表較直覺）\n",
    "    - 回傳 class_names, mapping, y_train01, y_test01\n",
    "    \"\"\"\n",
    "    all_vals = pd.concat([y_train.astype(str), y_test.astype(str)], ignore_index=True)\n",
    "    vc = all_vals.value_counts()\n",
    "    class_names = [str(cls) for cls in vc.index.tolist()][:2]\n",
    "    mapping = {cls: i for i, cls in enumerate(class_names)}\n",
    "    y_train01 = y_train.astype(str).map(mapping).to_numpy(dtype=int)\n",
    "    y_test01  = y_test.astype(str).map(mapping).to_numpy(dtype=int)\n",
    "    return class_names, mapping, y_train01, y_test01\n",
    "\n",
    "# ------------------ 手刻 ID3 / C4.5 ------------------\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    _, cnt = np.unique(y, return_counts=True)\n",
    "    p = cnt / cnt.sum()\n",
    "    return float(-(p * np.log2(p + 1e-12)).sum())\n",
    "\n",
    "def split_info(ns: List[int], n: int) -> float:\n",
    "    si = 0.0\n",
    "    for k in ns:\n",
    "        p = k / n\n",
    "        si -= p * math.log2(p + 1e-12)\n",
    "    return float(si)\n",
    "\n",
    "def choose_best_split_ID3(X, y, ftypes, min_leaf=1):\n",
    "    n, m = X.shape\n",
    "    base = entropy(y)\n",
    "    best, bj, obj, multi = -1.0, None, None, False\n",
    "    for j in range(m):\n",
    "        col = X[:, j]\n",
    "        if ftypes[j] == \"categorical\":\n",
    "            vals = np.unique(col)\n",
    "            parts = [np.where(col == v)[0] for v in vals]\n",
    "            if any(len(ix) < min_leaf for ix in parts): \n",
    "                continue\n",
    "            h = sum((len(ix)/n)*entropy(y[ix]) for ix in parts)\n",
    "            gain = base - h\n",
    "            if (gain > best + 1e-12) or (abs(gain-best)<=1e-12 and (bj is None or j<bj)):\n",
    "                best, bj, obj, multi = gain, j, {v:ix for v,ix in zip(vals, parts)}, True\n",
    "        else:\n",
    "            xs = np.unique(col.astype(float))\n",
    "            if len(xs) <= 1: \n",
    "                continue\n",
    "            ths = (xs[:-1] + xs[1:]) / 2.0\n",
    "            for t in ths:\n",
    "                L = np.where(col.astype(float) <= t)[0]\n",
    "                R = np.where(col.astype(float) >  t)[0]\n",
    "                if len(L)<min_leaf or len(R)<min_leaf: \n",
    "                    continue\n",
    "                h = (len(L)/n)*entropy(y[L]) + (len(R)/n)*entropy(y[R])\n",
    "                gain = base - h\n",
    "                if (gain > best + 1e-12) or (abs(gain-best)<=1e-12 and (bj is None or j<bj)):\n",
    "                    best, bj, obj, multi = gain, j, float(t), False\n",
    "    return bj, obj, best, multi\n",
    "\n",
    "def choose_best_split_C45(X, y, ftypes, min_leaf=1):\n",
    "    n, m = X.shape\n",
    "    base = entropy(y)\n",
    "    best, bj, obj, multi = -1.0, None, None, False\n",
    "    for j in range(m):\n",
    "        col = X[:, j]\n",
    "        if ftypes[j] == \"categorical\":\n",
    "            vals = np.unique(col)\n",
    "            parts = [np.where(col == v)[0] for v in vals]\n",
    "            if any(len(ix) < min_leaf for ix in parts): \n",
    "                continue\n",
    "            h = sum((len(ix)/n)*entropy(y[ix]) for ix in parts)\n",
    "            gain = base - h\n",
    "            gr = gain / (split_info([len(ix) for ix in parts], n) + 1e-12)\n",
    "            if (gr > best + 1e-12) or (abs(gr-best)<=1e-12 and (bj is None or j<bj)):\n",
    "                best, bj, obj, multi = gr, j, {v:ix for v,ix in zip(vals, parts)}, True\n",
    "        else:\n",
    "            xs = np.unique(col.astype(float))\n",
    "            if len(xs) <= 1: \n",
    "                continue\n",
    "            ths = (xs[:-1] + xs[1:]) / 2.0\n",
    "            for t in ths:\n",
    "                L = np.where(col.astype(float) <= t)[0]\n",
    "                R = np.where(col.astype(float) >  t)[0]\n",
    "                if len(L)<min_leaf or len(R)<min_leaf: \n",
    "                    continue\n",
    "                h = (len(L)/n)*entropy(y[L]) + (len(R)/n)*entropy(y[R])\n",
    "                gain = base - h\n",
    "                gr = gain / (split_info([len(L), len(R)], n) + 1e-12)\n",
    "                if (gr > best + 1e-12) or (abs(gr-best)<=1e-12 and (bj is None or j<bj)):\n",
    "                    best, bj, obj, multi = gr, j, float(t), False\n",
    "    return bj, obj, best, multi\n",
    "\n",
    "def majority_class(y):\n",
    "    cnt = Counter(y)\n",
    "    return int(sorted(cnt.items(), key=lambda kv:(-kv[1], kv[0]))[0][0])\n",
    "\n",
    "def build_tree(X, y, fnames, ftypes,\n",
    "               max_depth=8, min_samples_split=2, min_leaf=1,\n",
    "               depth=0, splitter=\"id3\", min_gain=1e-12):\n",
    "    n, m = X.shape\n",
    "    node = {\"n\": int(n)}\n",
    "    if depth>=max_depth or n<min_samples_split or len(np.unique(y))==1:\n",
    "        node.update({\"type\":\"leaf\",\"pred\": majority_class(y)})\n",
    "        return node\n",
    "\n",
    "    if splitter==\"id3\":\n",
    "        j, obj, score, multi = choose_best_split_ID3(X,y,ftypes,min_leaf)\n",
    "    else:\n",
    "        j, obj, score, multi = choose_best_split_C45(X,y,ftypes,min_leaf)\n",
    "\n",
    "    # 預剪枝門檻：若無可分裂或收益小於 min_gain，停止\n",
    "    if (j is None) or (score < min_gain):\n",
    "        node.update({\"type\":\"leaf\",\"pred\": majority_class(y)})\n",
    "        return node\n",
    "\n",
    "    if depth<=2:\n",
    "        tag = \"ID3\" if splitter==\"id3\" else \"C4.5\"\n",
    "        if multi: print(f\"[{tag}] depth={depth} feat={fnames[j]} (multiway) metric={score:.6f} n={n}\")\n",
    "        else:     print(f\"[{tag}] depth={depth} feat={fnames[j]} thr={obj:.6f} metric={score:.6f} n={n}\")\n",
    "\n",
    "    node.update({\"type\":\"node\",\"feature\":int(j)})\n",
    "    if multi:\n",
    "        kids={}\n",
    "        for v,idxs in obj.items():\n",
    "            kids[v] = build_tree(X[idxs,:], y[idxs], fnames, ftypes,\n",
    "                                 max_depth, min_samples_split, min_leaf,\n",
    "                                 depth+1, splitter, min_gain=min_gain)\n",
    "        node[\"children\"]=kids\n",
    "    else:\n",
    "        t=float(obj); col=X[:,j].astype(float)\n",
    "        L=np.where(col<=t)[0]; R=np.where(col>t)[0]\n",
    "        if len(L)==0 or len(R)==0:\n",
    "            node.update({\"type\":\"leaf\",\"pred\": majority_class(y)}); return node\n",
    "        node[\"threshold\"]=t\n",
    "        node[\"left\"]=build_tree(X[L,:], y[L], fnames, ftypes,\n",
    "                                max_depth, min_samples_split, min_leaf,\n",
    "                                depth+1, splitter, min_gain=min_gain)\n",
    "        node[\"right\"]=build_tree(X[R,:], y[R], fnames, ftypes,\n",
    "                                 max_depth, min_samples_split, min_leaf,\n",
    "                                 depth+1, splitter, min_gain=min_gain)\n",
    "    return node\n",
    "\n",
    "def predict_one(root, x):\n",
    "    node=root\n",
    "    while node.get(\"type\")==\"node\":\n",
    "        j=node[\"feature\"]\n",
    "        if \"children\" in node:\n",
    "            v=x[j]; node=node[\"children\"].get(v, None)\n",
    "            if node is None: return 0\n",
    "        else:\n",
    "            node = node[\"left\"] if float(x[j])<=node[\"threshold\"] else node[\"right\"]\n",
    "    return int(node[\"pred\"])\n",
    "\n",
    "def predict_batch(root, X):\n",
    "    return np.array([predict_one(root, X[i,:]) for i in range(X.shape[0])], dtype=int)\n",
    "\n",
    "# ----------- 修剪(+僅輸出前K層)與繪圖 -----------\n",
    "def clone_prune_to_depth(node, max_depth, depth=0):\n",
    "    if node[\"type\"]==\"leaf\" or depth>=max_depth:\n",
    "        return {\"type\":\"leaf\",\"pred\": node.get(\"pred\",0), \"n\": node.get(\"n\",0)}\n",
    "    new={\"type\":\"node\",\"feature\": node[\"feature\"], \"n\": node.get(\"n\",0)}\n",
    "    if \"children\" in node:\n",
    "        new[\"children\"]={k: clone_prune_to_depth(ch, max_depth, depth+1) for k,ch in node[\"children\"].items()}\n",
    "    else:\n",
    "        new[\"threshold\"]=node[\"threshold\"]\n",
    "        new[\"left\"]=clone_prune_to_depth(node[\"left\"], max_depth, depth+1)\n",
    "        new[\"right\"]=clone_prune_to_depth(node[\"right\"], max_depth, depth+1)\n",
    "    return new\n",
    "\n",
    "def to_dot(node, feature_names, class_names):\n",
    "    lines=['digraph Tree {','node [shape=box, fontname=\"Helvetica\"];','edge [fontname=\"Helvetica\"];']\n",
    "    nid=[0]\n",
    "    def walk(n):\n",
    "        my=nid[0]; nid[0]+=1\n",
    "        if n.get(\"type\")==\"leaf\":\n",
    "            lbl=f'leaf\\\\nclass={class_names[int(n.get(\"pred\",0))]}\\\\nsamples={n.get(\"n\",\"?\")}'\n",
    "            lines.append(f'{my} [label=\"{lbl}\", style=\"rounded,filled\"];'); return my\n",
    "        if \"children\" in n:\n",
    "            lbl=f'{feature_names[n[\"feature\"]]}'; lines.append(f'{my} [label=\"{lbl}\"];')\n",
    "            for cat,ch in n[\"children\"].items():\n",
    "                cid=walk(ch); cat_str=str(cat).replace('\"','\\\\\"'); lines.append(f'{my} -> {cid} [label=\"{cat_str}\"];')\n",
    "        else:\n",
    "            th=n[\"threshold\"]; lbl=f'{feature_names[n[\"feature\"]]} <= {th:.4f}'; lines.append(f'{my} [label=\"{lbl}\"];')\n",
    "            L=walk(n[\"left\"]); R=walk(n[\"right\"])\n",
    "            lines.append(f'{my} -> {L} [label=\"True\"];'); lines.append(f'{my} -> {R} [label=\"False\"];')\n",
    "        return my\n",
    "    walk(node); lines.append('}'); return \"\\n\".join(lines)\n",
    "\n",
    "def _count_leaves(n):\n",
    "    if n[\"type\"]==\"leaf\": return 1\n",
    "    if \"children\" in n: return sum(_count_leaves(ch) for ch in n[\"children\"].values())\n",
    "    return _count_leaves(n[\"left\"])+_count_leaves(n[\"right\"])\n",
    "\n",
    "def _layout(n, x0, x1, y, depth_step, coords, edges, edge_labels):\n",
    "    my_id=id(n); coords[my_id]=( (x0+x1)/2, y, n )\n",
    "    if n[\"type\"]==\"leaf\": return\n",
    "    if \"children\" in n:\n",
    "        kids=list(n[\"children\"].items())\n",
    "        sizes=[_count_leaves(ch) for _,ch in kids]; total=sum(sizes)\n",
    "        cur=x0\n",
    "        for (lab,ch),sz in zip(kids,sizes):\n",
    "            w=(x1-x0)*sz/total; nx0, nx1 = cur, cur+w; cur+=w\n",
    "            _layout(ch, nx0, nx1, y-depth_step, depth_step, coords, edges, edge_labels)\n",
    "            edges.append((my_id, id(ch))); edge_labels[(my_id, id(ch))]=str(lab)\n",
    "    else:\n",
    "        left,right = n[\"left\"], n[\"right\"]\n",
    "        sizes=[_count_leaves(left), _count_leaves(right)]; total=sum(sizes)\n",
    "        wL=(x1-x0)*sizes[0]/total; nx0, nx1 = x0, x0+wL\n",
    "        _layout(left, nx0, nx1, y-depth_step, depth_step, coords, edges, edge_labels)\n",
    "        _layout(right, x0+wL, x1, y-depth_step, depth_step, coords, edges, edge_labels)\n",
    "        edges.append((my_id, id(left)));  edge_labels[(my_id,id(left))]=\"True\"\n",
    "        edges.append((my_id, id(right))); edge_labels[(my_id,id(right))]=\"False\"\n",
    "\n",
    "def save_handmade_tree_png(root, feature_names, class_names, out_png, view_max_depth=VIEW_DEPTH):\n",
    "    pruned = clone_prune_to_depth(root, view_max_depth)\n",
    "    if _GRAPHVIZ_OK:\n",
    "        dot = to_dot(pruned, feature_names, class_names)\n",
    "        src = Source(dot); src.format=\"png\"; src.render(out_png.replace(\".png\",\"\"), cleanup=True)\n",
    "        print(f\"[OK] 手刻樹圖片：{out_png}\")\n",
    "        return\n",
    "    # 後備：Matplotlib\n",
    "    coords, edges, edge_labels = {}, [], {}\n",
    "    _layout(pruned, 0.0, 1.0, 1.0, 0.18, coords, edges, edge_labels)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for (x,y,nid) in [(x,y,n) for (x,y,n) in [(*coords[k][:2], coords[k][2]) for k in coords]]:\n",
    "        n = nid\n",
    "        if n[\"type\"]==\"leaf\":\n",
    "            txt = f'leaf\\nclass={class_names[int(n.get(\"pred\",0))]}\\nN={n.get(\"n\",\"?\")}'\n",
    "        else:\n",
    "            if \"children\" in n:\n",
    "                txt = feature_names[n[\"feature\"]]\n",
    "            else:\n",
    "                txt = f'{feature_names[n[\"feature\"]]} <= {n[\"threshold\"]:.3f}'\n",
    "        ax.text(x, y, txt, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"w\", ec=\"k\"))\n",
    "    for (u,v) in edges:\n",
    "        x1,y1,_ = coords[u]; x2,y2,_ = coords[v]\n",
    "        ax.annotate(\"\", xy=(x2,y2+0.01), xytext=(x1,y1-0.01), arrowprops=dict(arrowstyle=\"-\"))\n",
    "        lbl = edge_labels[(u,v)]\n",
    "        ax.text((x1+x2)/2, (y1+y2)/2, lbl, ha=\"center\", va=\"center\")\n",
    "    ax.set_axis_off(); plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "    print(f\"[OK] 手刻樹圖片(後備)：{out_png}\")\n",
    "\n",
    "# ---------------- 評估 ----------------\n",
    "def save_cm_png(y_true01, y_pred01, class_names, title, out_png):\n",
    "    cm = confusion_matrix(y_true01, y_pred01, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "    print(f\"[OK] 混淆矩陣：{out_png}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 主流程（固定標籤為 class）\n",
    "# ===============================================================\n",
    "\n",
    "train_csv = \"adult_data_no_duplicates.csv\"\n",
    "test_csv  = \"adult_test_no_duplicates.csv\"\n",
    "\n",
    "# 0) 讀檔 & 基礎清理\n",
    "train_df, test_df = load_csvs(train_csv, test_csv)\n",
    "train_df = standardize_colnames(train_df)\n",
    "test_df  = standardize_colnames(test_df)\n",
    "train_df = strip_object_columns(train_df)\n",
    "test_df  = strip_object_columns(test_df)\n",
    "\n",
    "# 1) 固定標籤名稱（含 calss → class 防呆）\n",
    "for name, df in [('train', train_df), ('test', test_df)]:\n",
    "    if 'class' not in df.columns and 'calss' in df.columns:\n",
    "        print(f\"[Info] 偵測到 {name} 標籤欄位：calss -> 重新命名為 class\")\n",
    "        df.rename(columns={'calss': 'class'}, inplace=True)\n",
    "    if 'class' not in df.columns:\n",
    "        raise KeyError(f\"{name}_df 缺少 'class' 欄位，現有欄位：{df.columns.tolist()}\")\n",
    "TARGET = 'class'\n",
    "\n",
    "# 2) 清理標籤內容（去句點/空白）\n",
    "train_df[TARGET] = _clean_label_series(train_df[TARGET])\n",
    "test_df[TARGET]  = _clean_label_series(test_df[TARGET])\n",
    "\n",
    "# 3) 數值轉型 + 缺值補\n",
    "train_df = coerce_numeric_columns(train_df, exclude=[TARGET])\n",
    "test_df  = coerce_numeric_columns(test_df,  exclude=[TARGET])\n",
    "train_df = basic_impute(train_df, TARGET)\n",
    "test_df  = basic_impute(test_df,  TARGET)\n",
    "\n",
    "# 4) 拆 X / y 與欄型\n",
    "X_train_df, y_train_sr = split_xy(train_df, TARGET)\n",
    "X_test_df,  y_test_sr  = split_xy(test_df,  TARGET)\n",
    "feature_names = X_train_df.columns.tolist()\n",
    "feature_types = get_feature_types(X_train_df)\n",
    "\n",
    "# 5) 標籤編碼（多數類為 class_names[0] -> 0）\n",
    "class_names, mapping, y_train01, y_test01 = make_label_encoder(y_train_sr, y_test_sr)\n",
    "print(f\"[Info] class_names = {class_names}  (0 -> {class_names[0]}, 1 -> {class_names[1]})\")\n",
    "\n",
    "# 6) 轉 numpy（手刻樹用）\n",
    "X_train_np = to_numpy_mixed(X_train_df)\n",
    "X_test_np  = to_numpy_mixed(X_test_df)\n",
    "\n",
    "# ===============================================================\n",
    "# 自動偵測過擬合 + 自動調參（ID3 / C4.5 / CART）\n",
    "# 判斷：ΔAcc = Train_Acc - Test_Acc；若 ΔAcc > GAP_THRESHOLD 視為過擬合\n",
    "# 調參（ID3/C4.5）：提高 min_gain、提高 min_samples_split、降低 max_depth（預剪枝）\n",
    "# 調參（CART）：提高 min_impurity_decrease、提高 min_samples_split、降低 max_depth、\n",
    "#               降低 max_leaf_nodes，並嘗試 cost-complexity 後剪枝 (ccp_alpha)\n",
    "# ===============================================================\n",
    "\n",
    "# ---- 門檻設定：固定 2.5% ----\n",
    "GAP_THRESHOLD = 0.025\n",
    "\n",
    "MAX_TUNE_STEPS  = 5  # 每個模型最多調參步數\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_tree(root):\n",
    "    \"\"\"回傳 train_acc, test_acc, Δacc, y_train_pred, y_test_pred for handmade trees.\"\"\"\n",
    "    y_tr = predict_batch(root, X_train_np)\n",
    "    y_te = predict_batch(root, X_test_np)\n",
    "    tr = accuracy_score(y_train01, y_tr)\n",
    "    te = accuracy_score(y_test01,  y_te)\n",
    "    return tr, te, tr-te, y_tr, y_te\n",
    "\n",
    "def autotune_id3():\n",
    "    params = dict(max_depth=TRAIN_MAX_DEPTH,\n",
    "                  min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "                  min_gain=MIN_GAIN)\n",
    "    history = []\n",
    "    root = None\n",
    "    for step in range(1, MAX_TUNE_STEPS+1):\n",
    "        root = build_tree(X_train_np, y_train01, feature_names, feature_types,\n",
    "                          max_depth=params[\"max_depth\"],\n",
    "                          min_samples_split=params[\"min_samples_split\"],\n",
    "                          min_leaf=1, splitter=\"id3\", min_gain=params[\"min_gain\"])\n",
    "        tr, te, gap, _, _ = eval_tree(root)\n",
    "        history.append((step, tr, te, gap, params.copy()))\n",
    "        print(f\"[ID3][step {step}] Train={tr:.4f} Test={te:.4f} Δ={gap:.4f}  params={params}\")\n",
    "        if gap <= GAP_THRESHOLD:\n",
    "            break\n",
    "        # ---- 預剪枝變嚴 ----\n",
    "        params[\"min_gain\"] = min(params[\"min_gain\"]*2.0, 1e-1)\n",
    "        params[\"min_samples_split\"] = min(int(params[\"min_samples_split\"]*1.25), 200)\n",
    "        params[\"max_depth\"] = max(3, params[\"max_depth\"]-1)\n",
    "    return root, params, history\n",
    "\n",
    "def autotune_c45():\n",
    "    params = dict(max_depth=TRAIN_MAX_DEPTH,\n",
    "                  min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "                  min_gain=MIN_GAIN)\n",
    "    history = []\n",
    "    root = None\n",
    "    for step in range(1, MAX_TUNE_STEPS+1):\n",
    "        root = build_tree(X_train_np, y_train01, feature_names, feature_types,\n",
    "                          max_depth=params[\"max_depth\"],\n",
    "                          min_samples_split=params[\"min_samples_split\"],\n",
    "                          min_leaf=1, splitter=\"c45\", min_gain=params[\"min_gain\"])\n",
    "        tr, te, gap, _, _ = eval_tree(root)\n",
    "        history.append((step, tr, te, gap, params.copy()))\n",
    "        print(f\"[C4.5][step {step}] Train={tr:.4f} Test={te:.4f} Δ={gap:.4f}  params={params}\")\n",
    "        if gap <= GAP_THRESHOLD:\n",
    "            break\n",
    "        # ---- 預剪枝變嚴 ----\n",
    "        params[\"min_gain\"] = min(params[\"min_gain\"]*1.7, 1e-1)\n",
    "        params[\"min_samples_split\"] = min(int(params[\"min_samples_split\"]*1.2), 200)\n",
    "        params[\"max_depth\"] = max(3, params[\"max_depth\"]-1)\n",
    "    return root, params, history\n",
    "\n",
    "def train_cart_with_params(p):\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=\"gini\",\n",
    "        max_depth=p[\"max_depth\"],\n",
    "        min_samples_split=p[\"min_samples_split\"],\n",
    "        min_impurity_decrease=p[\"min_impurity_decrease\"],\n",
    "        max_leaf_nodes=p[\"max_leaf_nodes\"],\n",
    "        ccp_alpha=p.get(\"ccp_alpha\", 0.0),\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model.fit(Xtr_cart, y_train01)\n",
    "    y_tr = model.predict(Xtr_cart)\n",
    "    y_te = model.predict(Xte_cart)\n",
    "    tr = accuracy_score(y_train01, y_tr)\n",
    "    te = accuracy_score(y_test01,  y_te)\n",
    "    gap = tr - te\n",
    "    return model, (tr, te, gap, y_tr, y_te)\n",
    "\n",
    "def autotune_cart():\n",
    "    params = dict(max_depth=TRAIN_MAX_DEPTH,\n",
    "                  min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "                  min_impurity_decrease=MIN_GAIN,\n",
    "                  max_leaf_nodes=MAX_LEAF_NODES,\n",
    "                  ccp_alpha=0.0)\n",
    "    history = []\n",
    "    model = None\n",
    "    for step in range(1, MAX_TUNE_STEPS+1):\n",
    "        # 嘗試取 cost-complexity 路徑的 25 分位作為溫和後剪枝\n",
    "        try:\n",
    "            from sklearn.tree import DecisionTreeClassifier as _DTC\n",
    "            path = _DTC(random_state=SEED).cost_complexity_pruning_path(Xtr_cart, y_train01)\n",
    "            alpha_q25 = float(np.quantile(path.ccp_alphas, 0.25))\n",
    "            params[\"ccp_alpha\"] = max(params.get(\"ccp_alpha\", 0.0), alpha_q25)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        model, stats = train_cart_with_params(params)\n",
    "        tr, te, gap, _, _ = stats\n",
    "        history.append((step, tr, te, gap, params.copy()))\n",
    "        print(f\"[CART][step {step}] Train={tr:.4f} Test={te:.4f} Δ={gap:.4f}  params={params}\")\n",
    "        if gap <= GAP_THRESHOLD:\n",
    "            break\n",
    "        # ---- 預剪枝更嚴＋後剪枝 ----\n",
    "        params[\"min_impurity_decrease\"] = min(params[\"min_impurity_decrease\"]*2.0, 1e-1)\n",
    "        params[\"min_samples_split\"] = min(int(params[\"min_samples_split\"]*1.25), 300)\n",
    "        params[\"max_depth\"] = max(3, params[\"max_depth\"]-1)\n",
    "        params[\"max_leaf_nodes\"] = max(16, int(params[\"max_leaf_nodes\"]*0.8))\n",
    "        params[\"ccp_alpha\"] = min(params.get(\"ccp_alpha\", 0.0)*1.5 + 1e-5, 1e-1)\n",
    "    return model, params, history\n",
    "\n",
    "# === CART 的前處理（One-Hot 等） ===\n",
    "cat_cols = [c for c,t in zip(feature_names, feature_types) if t==\"categorical\"]\n",
    "num_cols = [c for c,t in zip(feature_names, feature_types) if t==\"numeric\"]\n",
    "cart_pre = ColumnTransformer(\n",
    "    [(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "     (\"num\", \"passthrough\", num_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "Xtr_cart = cart_pre.fit_transform(X_train_df)\n",
    "Xte_cart = cart_pre.transform(X_test_df)\n",
    "\n",
    "# === 自動調參訓練 ===\n",
    "print(\"\\n=== 自動調參：ID3 ===\")\n",
    "id3_root, id3_params, id3_hist = autotune_id3()\n",
    "\n",
    "print(\"\\n=== 自動調參：C4.5 ===\")\n",
    "c45_root, c45_params, c45_hist = autotune_c45()\n",
    "\n",
    "print(\"\\n=== 自動調參：CART ===\")\n",
    "cart, cart_params, cart_hist = autotune_cart()\n",
    "\n",
    "# ---- 樹圖（僅前 VIEW_DEPTH 層）----\n",
    "print(\"\\n=== 輸出樹圖（僅前 3 層） ===\")\n",
    "save_handmade_tree_png(id3_root, feature_names, class_names, \"Tree_ID3.png\", view_max_depth=VIEW_DEPTH)\n",
    "save_handmade_tree_png(c45_root, feature_names, class_names, \"Tree_C45.png\", view_max_depth=VIEW_DEPTH)\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plot_tree(cart,\n",
    "          feature_names=cart_pre.get_feature_names_out(),\n",
    "          class_names=class_names,\n",
    "          filled=True, max_depth=VIEW_DEPTH, impurity=False)\n",
    "plt.tight_layout(); plt.savefig(\"Tree_CART.png\", dpi=300); plt.close()\n",
    "print(\"[OK] sklearn 樹圖片：Tree_CART.png\")\n",
    "\n",
    "# ---- 最終評估（單一來源，避免不一致）----\n",
    "print(\"\\n=== 評估（最終模型）===\")\n",
    "\n",
    "# handmade trees\n",
    "id3_tr, id3_te, _, id3_ytr, id3_yte = eval_tree(id3_root)\n",
    "c45_tr, c45_te, _, c45_ytr, c45_yte = eval_tree(c45_root)\n",
    "# CART\n",
    "y_cart_train = cart.predict(Xtr_cart)\n",
    "y_cart_test  = cart.predict(Xte_cart)\n",
    "cart_tr = accuracy_score(y_train01, y_cart_train)\n",
    "cart_te = accuracy_score(y_test01,  y_cart_test)\n",
    "\n",
    "acc_df = pd.DataFrame([\n",
    "    (\"ID3\",  id3_tr,  id3_te),\n",
    "    (\"C4.5\", c45_tr,  c45_te),\n",
    "    (\"CART\", cart_tr, cart_te),\n",
    "], columns=[\"Model\",\"Train_Acc\",\"Test_Acc\"]).round(6)\n",
    "print(\"\\n=== Accuracy (Train / Test) ===\")\n",
    "print(acc_df.to_string(index=False))\n",
    "\n",
    "# classification report + 混淆矩陣（測試集）\n",
    "for name, yhat in [(\"ID3\", id3_yte), (\"C4.5\", c45_yte), (\"CART\", y_cart_test)]:\n",
    "    acc  = accuracy_score(y_test01, yhat)\n",
    "    print(f\"\\n[{name}] Accuracy = {acc:.4f}\")\n",
    "    print(classification_report(y_test01, yhat, target_names=class_names, digits=4))\n",
    "    save_cm_png(y_test01, yhat, class_names, name, f\"CM_{name.replace('.','')}.png\")\n",
    "\n",
    "# 參數調整紀錄輸出（方便寫報告）\n",
    "def hist_to_df(hist, model):\n",
    "    rows=[]\n",
    "    for step,tr,te,gap,params in hist:\n",
    "        rows.append({\"Model\":model,\"Step\":step,\"Train_Acc\":tr,\"Test_Acc\":te,\"Delta\":tr-te,**params})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tune_log = pd.concat([\n",
    "    hist_to_df(id3_hist,\"ID3\"),\n",
    "    hist_to_df(c45_hist,\"C4.5\"),\n",
    "    hist_to_df(cart_hist,\"CART\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "tune_log.round(6).to_csv(\"tune_log.csv\", index=False)\n",
    "acc_df.to_csv(\"accuracy_train_test.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== 完成 ===\")\n",
    "print(\"輸出：Tree_ID3.png / Tree_C45.png / Tree_CART.png / CM_ID3.png / CM_C45.png / CM_CART.png / accuracy_train_test.csv / tune_log.csv\")\n",
    "print(f\"[說明] 過擬合門檻 GAP_THRESHOLD = {GAP_THRESHOLD:.3f}（可改為 3×SE 的資料驅動版本；見下方參考）\")\n",
    "\n",
    "# ---- 參考：用 3×SE 給一個資料驅動的門檻建議（不會影響上面結果）----\n",
    "ser = acc_df.loc[acc_df[\"Model\"].eq(\"CART\"), \"Test_Acc\"]\n",
    "if not ser.empty:\n",
    "    p_hat = float(ser.iloc[0])            #  正確取標量\n",
    "else:\n",
    "    best_row = acc_df.iloc[acc_df[\"Test_Acc\"].idxmax()]\n",
    "    p_hat = float(best_row[\"Test_Acc\"])   # 退而求其次：用 Test_Acc 最好的模型\n",
    "\n",
    "n = len(y_test01)\n",
    "se = (p_hat * (1 - p_hat) / n) ** 0.5\n",
    "thr_se = max(0.02, 3 * se)  # 保底 2%\n",
    "\n",
    "print(f\"[參考] 以 p≈{p_hat:.4f}, n={n}, 3×SE≈{3*se:.4%}，資料驅動門檻建議 ≥ {thr_se:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b30af1-e91a-4a00-94b3-2b69258f20ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
